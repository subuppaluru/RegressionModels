{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AirQuality_Regression",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "FZ3cnD6LsDkQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkE96Lm43y3z"
      },
      "source": [
        "df = pd.read_excel('AirQualityUCI.xlsx')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCzYqqkqxtMc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2289098c-ffee-44e2-ede9-cbf379a5da00"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-05ccc783-0a86-45dd-9189-74231d959639\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>18:00:00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.00</td>\n",
              "      <td>150</td>\n",
              "      <td>11.881723</td>\n",
              "      <td>1045.50</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.25</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.00</td>\n",
              "      <td>1267.50</td>\n",
              "      <td>13.60</td>\n",
              "      <td>48.875001</td>\n",
              "      <td>0.757754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>19:00:00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.25</td>\n",
              "      <td>112</td>\n",
              "      <td>9.397165</td>\n",
              "      <td>954.75</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1173.75</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1558.75</td>\n",
              "      <td>972.25</td>\n",
              "      <td>13.30</td>\n",
              "      <td>47.700000</td>\n",
              "      <td>0.725487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>20:00:00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.00</td>\n",
              "      <td>88</td>\n",
              "      <td>8.997817</td>\n",
              "      <td>939.25</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.00</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1554.50</td>\n",
              "      <td>1074.00</td>\n",
              "      <td>11.90</td>\n",
              "      <td>53.975000</td>\n",
              "      <td>0.750239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>21:00:00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1375.50</td>\n",
              "      <td>80</td>\n",
              "      <td>9.228796</td>\n",
              "      <td>948.25</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1583.75</td>\n",
              "      <td>1203.25</td>\n",
              "      <td>11.00</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.786713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>22:00:00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.25</td>\n",
              "      <td>51</td>\n",
              "      <td>6.518224</td>\n",
              "      <td>835.50</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.00</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.00</td>\n",
              "      <td>1110.00</td>\n",
              "      <td>11.15</td>\n",
              "      <td>59.575001</td>\n",
              "      <td>0.788794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05ccc783-0a86-45dd-9189-74231d959639')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05ccc783-0a86-45dd-9189-74231d959639 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05ccc783-0a86-45dd-9189-74231d959639');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Date      Time  CO(GT)  ...      T         RH        AH\n",
              "0 2004-03-10  18:00:00     2.6  ...  13.60  48.875001  0.757754\n",
              "1 2004-03-10  19:00:00     2.0  ...  13.30  47.700000  0.725487\n",
              "2 2004-03-10  20:00:00     2.2  ...  11.90  53.975000  0.750239\n",
              "3 2004-03-10  21:00:00     2.2  ...  11.00  60.000000  0.786713\n",
              "4 2004-03-10  22:00:00     1.6  ...  11.15  59.575001  0.788794\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CUF48ea4F_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66040b6-f275-4151-dfb5-d28ad981aa66"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9357 entries, 0 to 9356\n",
            "Data columns (total 15 columns):\n",
            " #   Column         Non-Null Count  Dtype         \n",
            "---  ------         --------------  -----         \n",
            " 0   Date           9357 non-null   datetime64[ns]\n",
            " 1   Time           9357 non-null   object        \n",
            " 2   CO(GT)         9357 non-null   float64       \n",
            " 3   PT08.S1(CO)    9357 non-null   float64       \n",
            " 4   NMHC(GT)       9357 non-null   int64         \n",
            " 5   C6H6(GT)       9357 non-null   float64       \n",
            " 6   PT08.S2(NMHC)  9357 non-null   float64       \n",
            " 7   NOx(GT)        9357 non-null   float64       \n",
            " 8   PT08.S3(NOx)   9357 non-null   float64       \n",
            " 9   NO2(GT)        9357 non-null   float64       \n",
            " 10  PT08.S4(NO2)   9357 non-null   float64       \n",
            " 11  PT08.S5(O3)    9357 non-null   float64       \n",
            " 12  T              9357 non-null   float64       \n",
            " 13  RH             9357 non-null   float64       \n",
            " 14  AH             9357 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(12), int64(1), object(1)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2fb3ZSMsvHh",
        "outputId": "824f0806-50c6-40e9-a7f4-349b9ecf87a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9357, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsMltACB-1vA"
      },
      "source": [
        "X = df.drop(['Date','Time','AH'],axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw_3rnEwxTmI",
        "outputId": "4a318d85-c6f5-4a1d-96fe-89adf94eb147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ed8b9511-d8e7-4681-944b-a6b3ad8f9c5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.00</td>\n",
              "      <td>150</td>\n",
              "      <td>11.881723</td>\n",
              "      <td>1045.50</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.25</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.00</td>\n",
              "      <td>1267.50</td>\n",
              "      <td>13.60</td>\n",
              "      <td>48.875001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.25</td>\n",
              "      <td>112</td>\n",
              "      <td>9.397165</td>\n",
              "      <td>954.75</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1173.75</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1558.75</td>\n",
              "      <td>972.25</td>\n",
              "      <td>13.30</td>\n",
              "      <td>47.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.00</td>\n",
              "      <td>88</td>\n",
              "      <td>8.997817</td>\n",
              "      <td>939.25</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.00</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1554.50</td>\n",
              "      <td>1074.00</td>\n",
              "      <td>11.90</td>\n",
              "      <td>53.975000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1375.50</td>\n",
              "      <td>80</td>\n",
              "      <td>9.228796</td>\n",
              "      <td>948.25</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1583.75</td>\n",
              "      <td>1203.25</td>\n",
              "      <td>11.00</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.25</td>\n",
              "      <td>51</td>\n",
              "      <td>6.518224</td>\n",
              "      <td>835.50</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.00</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.00</td>\n",
              "      <td>1110.00</td>\n",
              "      <td>11.15</td>\n",
              "      <td>59.575001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed8b9511-d8e7-4681-944b-a6b3ad8f9c5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed8b9511-d8e7-4681-944b-a6b3ad8f9c5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed8b9511-d8e7-4681-944b-a6b3ad8f9c5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   CO(GT)  PT08.S1(CO)  NMHC(GT)  ...  PT08.S5(O3)      T         RH\n",
              "0     2.6      1360.00       150  ...      1267.50  13.60  48.875001\n",
              "1     2.0      1292.25       112  ...       972.25  13.30  47.700000\n",
              "2     2.2      1402.00        88  ...      1074.00  11.90  53.975000\n",
              "3     2.2      1375.50        80  ...      1203.25  11.00  60.000000\n",
              "4     1.6      1272.25        51  ...      1110.00  11.15  59.575001\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8OWHZ5YtOPp",
        "outputId": "bf4bff8d-f84c-4d9c-da41-71ed92fb84c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9357, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx9XePk_tRw_",
        "outputId": "7ec33efc-2759-427d-b806-97408298cc5e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9357 entries, 0 to 9356\n",
            "Data columns (total 12 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   CO(GT)         9357 non-null   float64\n",
            " 1   PT08.S1(CO)    9357 non-null   float64\n",
            " 2   NMHC(GT)       9357 non-null   int64  \n",
            " 3   C6H6(GT)       9357 non-null   float64\n",
            " 4   PT08.S2(NMHC)  9357 non-null   float64\n",
            " 5   NOx(GT)        9357 non-null   float64\n",
            " 6   PT08.S3(NOx)   9357 non-null   float64\n",
            " 7   NO2(GT)        9357 non-null   float64\n",
            " 8   PT08.S4(NO2)   9357 non-null   float64\n",
            " 9   PT08.S5(O3)    9357 non-null   float64\n",
            " 10  T              9357 non-null   float64\n",
            " 11  RH             9357 non-null   float64\n",
            "dtypes: float64(11), int64(1)\n",
            "memory usage: 877.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOY8H1rt-9PY"
      },
      "source": [
        "y = df['AH']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHkPv55txfd3",
        "outputId": "efa560b2-fd5d-4898-b49b-1b17afccea15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y[0:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.757754\n",
              "1    0.725487\n",
              "2    0.750239\n",
              "3    0.786713\n",
              "4    0.788794\n",
              "Name: AH, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correlation"
      ],
      "metadata": {
        "id": "LLzDHbXItguY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "corr = X.corr()"
      ],
      "metadata": {
        "id": "9yRpc3HQthtX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "bYbfXqprtmiw",
        "outputId": "740897d7-9606-4b0a-a509-67a2c74f5dd1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-62a0ab15-5097-4ff4-9387-f35c19329a7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>CO(GT)</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.041415</td>\n",
              "      <td>0.128351</td>\n",
              "      <td>-0.031377</td>\n",
              "      <td>0.029939</td>\n",
              "      <td>0.526450</td>\n",
              "      <td>-0.089981</td>\n",
              "      <td>0.671140</td>\n",
              "      <td>-0.073721</td>\n",
              "      <td>0.080316</td>\n",
              "      <td>-0.068952</td>\n",
              "      <td>-0.048231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <td>0.041415</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.170009</td>\n",
              "      <td>0.852659</td>\n",
              "      <td>0.933101</td>\n",
              "      <td>0.278029</td>\n",
              "      <td>0.086931</td>\n",
              "      <td>0.154058</td>\n",
              "      <td>0.845133</td>\n",
              "      <td>0.892436</td>\n",
              "      <td>0.754806</td>\n",
              "      <td>0.745344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <td>0.128351</td>\n",
              "      <td>0.170009</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.037329</td>\n",
              "      <td>0.110097</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>0.048832</td>\n",
              "      <td>0.103345</td>\n",
              "      <td>0.162689</td>\n",
              "      <td>0.101189</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.008288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <td>-0.031377</td>\n",
              "      <td>0.852659</td>\n",
              "      <td>0.037329</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.767401</td>\n",
              "      <td>-0.001163</td>\n",
              "      <td>0.512154</td>\n",
              "      <td>-0.010971</td>\n",
              "      <td>0.774649</td>\n",
              "      <td>0.641306</td>\n",
              "      <td>0.971370</td>\n",
              "      <td>0.925068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <td>0.029939</td>\n",
              "      <td>0.933101</td>\n",
              "      <td>0.110097</td>\n",
              "      <td>0.767401</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.331331</td>\n",
              "      <td>-0.073748</td>\n",
              "      <td>0.176569</td>\n",
              "      <td>0.874761</td>\n",
              "      <td>0.909909</td>\n",
              "      <td>0.668984</td>\n",
              "      <td>0.585775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NOx(GT)</th>\n",
              "      <td>0.526450</td>\n",
              "      <td>0.278029</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>-0.001163</td>\n",
              "      <td>0.331331</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.436083</td>\n",
              "      <td>0.817138</td>\n",
              "      <td>0.035580</td>\n",
              "      <td>0.461916</td>\n",
              "      <td>-0.138457</td>\n",
              "      <td>-0.053008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <td>-0.089981</td>\n",
              "      <td>0.086931</td>\n",
              "      <td>0.048832</td>\n",
              "      <td>0.512154</td>\n",
              "      <td>-0.073748</td>\n",
              "      <td>-0.436083</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.256217</td>\n",
              "      <td>0.122672</td>\n",
              "      <td>-0.208935</td>\n",
              "      <td>0.588061</td>\n",
              "      <td>0.573513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NO2(GT)</th>\n",
              "      <td>0.671140</td>\n",
              "      <td>0.154058</td>\n",
              "      <td>0.103345</td>\n",
              "      <td>-0.010971</td>\n",
              "      <td>0.176569</td>\n",
              "      <td>0.817138</td>\n",
              "      <td>-0.256217</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.022092</td>\n",
              "      <td>0.253469</td>\n",
              "      <td>-0.084084</td>\n",
              "      <td>-0.081300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <td>-0.073721</td>\n",
              "      <td>0.845133</td>\n",
              "      <td>0.162689</td>\n",
              "      <td>0.774649</td>\n",
              "      <td>0.874761</td>\n",
              "      <td>0.035580</td>\n",
              "      <td>0.122672</td>\n",
              "      <td>-0.022092</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.723670</td>\n",
              "      <td>0.755053</td>\n",
              "      <td>0.640685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <td>0.080316</td>\n",
              "      <td>0.892436</td>\n",
              "      <td>0.101189</td>\n",
              "      <td>0.641306</td>\n",
              "      <td>0.909909</td>\n",
              "      <td>0.461916</td>\n",
              "      <td>-0.208935</td>\n",
              "      <td>0.253469</td>\n",
              "      <td>0.723670</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.503659</td>\n",
              "      <td>0.524922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>-0.068952</td>\n",
              "      <td>0.754806</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.971370</td>\n",
              "      <td>0.668984</td>\n",
              "      <td>-0.138457</td>\n",
              "      <td>0.588061</td>\n",
              "      <td>-0.084084</td>\n",
              "      <td>0.755053</td>\n",
              "      <td>0.503659</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.885910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RH</th>\n",
              "      <td>-0.048231</td>\n",
              "      <td>0.745344</td>\n",
              "      <td>0.008288</td>\n",
              "      <td>0.925068</td>\n",
              "      <td>0.585775</td>\n",
              "      <td>-0.053008</td>\n",
              "      <td>0.573513</td>\n",
              "      <td>-0.081300</td>\n",
              "      <td>0.640685</td>\n",
              "      <td>0.524922</td>\n",
              "      <td>0.885910</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62a0ab15-5097-4ff4-9387-f35c19329a7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-62a0ab15-5097-4ff4-9387-f35c19329a7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-62a0ab15-5097-4ff4-9387-f35c19329a7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 CO(GT)  PT08.S1(CO)  NMHC(GT)  ...  PT08.S5(O3)         T        RH\n",
              "CO(GT)         1.000000     0.041415  0.128351  ...     0.080316 -0.068952 -0.048231\n",
              "PT08.S1(CO)    0.041415     1.000000  0.170009  ...     0.892436  0.754806  0.745344\n",
              "NMHC(GT)       0.128351     0.170009  1.000000  ...     0.101189 -0.000008  0.008288\n",
              "C6H6(GT)      -0.031377     0.852659  0.037329  ...     0.641306  0.971370  0.925068\n",
              "PT08.S2(NMHC)  0.029939     0.933101  0.110097  ...     0.909909  0.668984  0.585775\n",
              "NOx(GT)        0.526450     0.278029 -0.004413  ...     0.461916 -0.138457 -0.053008\n",
              "PT08.S3(NOx)  -0.089981     0.086931  0.048832  ...    -0.208935  0.588061  0.573513\n",
              "NO2(GT)        0.671140     0.154058  0.103345  ...     0.253469 -0.084084 -0.081300\n",
              "PT08.S4(NO2)  -0.073721     0.845133  0.162689  ...     0.723670  0.755053  0.640685\n",
              "PT08.S5(O3)    0.080316     0.892436  0.101189  ...     1.000000  0.503659  0.524922\n",
              "T             -0.068952     0.754806 -0.000008  ...     0.503659  1.000000  0.885910\n",
              "RH            -0.048231     0.745344  0.008288  ...     0.524922  0.885910  1.000000\n",
              "\n",
              "[12 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(\n",
        "    corr, \n",
        "    vmin=-1, vmax=1, center=0,\n",
        "    cmap=sns.diverging_palette(20, 220, n=200),\n",
        "    square=True\n",
        ")\n",
        "\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation=45,\n",
        "    horizontalalignment='right'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "vRRd1QlgtsbP",
        "outputId": "960912d0-0de1-453a-9145-d0df590e7b0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 0, 'CO(GT)'),\n",
              " Text(1.5, 0, 'PT08.S1(CO)'),\n",
              " Text(2.5, 0, 'NMHC(GT)'),\n",
              " Text(3.5, 0, 'C6H6(GT)'),\n",
              " Text(4.5, 0, 'PT08.S2(NMHC)'),\n",
              " Text(5.5, 0, 'NOx(GT)'),\n",
              " Text(6.5, 0, 'PT08.S3(NOx)'),\n",
              " Text(7.5, 0, 'NO2(GT)'),\n",
              " Text(8.5, 0, 'PT08.S4(NO2)'),\n",
              " Text(9.5, 0, 'PT08.S5(O3)'),\n",
              " Text(10.5, 0, 'T'),\n",
              " Text(11.5, 0, 'RH')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(corr, annot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "ddctPB_RtwyX",
        "outputId": "d457c738-44ca-4dbb-d2bc-3954ff1d53f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#outliers"
      ],
      "metadata": {
        "id": "K_NEn6dFt3zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "z = np.abs(stats.zscore(X))\n",
        "print(z)\n",
        "print(z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVDH5ZB2t4lJ",
        "outputId": "0421a7be-2c82-4779-cfe2-818d7d6b705b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.4739999  0.94339273 2.21123554 ... 0.64029229 0.08850233 0.18337935]\n",
            " [0.46627322 0.73796482 1.93938293 ... 0.00591277 0.08155806 0.16043589]\n",
            " [0.46884878 1.07074288 1.76768654 ... 0.21678449 0.04915151 0.28296361]\n",
            " ...\n",
            " [0.47142434 0.28238484 0.29267014 ... 0.25618056 0.39636464 0.41266181]\n",
            " [0.467561   0.14059957 0.29267014 ... 0.44911861 0.4293499  0.50638818]\n",
            " [0.46884878 0.06634442 0.29267014 ... 0.34789258 0.43340073 0.51468687]]\n",
            "(9357, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 3\n",
        "print(np.where(z > 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3g5gUHvuAjO",
        "outputId": "7f7dc1f3-d903-416f-980e-e4941026a237"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([  10,   11,   24, ..., 8777, 8777, 8777]), array([ 6,  6,  2, ...,  8, 10, 11]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.where(z < 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lq0-sKYuDLj",
        "outputId": "d59e0b0a-7093-49fe-a93a-164be45a1e05"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([   0,    0,    0, ..., 9356, 9356, 9356]), array([ 0,  1,  2, ...,  9, 10, 11]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outliers: Correcting, Removing"
      ],
      "metadata": {
        "id": "gwPUZx7puOSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_o = X[(z < 3).all(axis=1)]\n",
        "X_i = X[~((z < 3).all(axis=1))]\n",
        "\n",
        "print(\"Original Dataset Shape: \",X.shape)\n",
        "print(\"Outlier in the Dataset Shape: \",X_o.shape)\n",
        "print(\"Without Outlier in the Dataset Shape: \",X_i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NyCLtzUuMQi",
        "outputId": "b9752dbd-9e1e-45b3-ed55-04377129e022"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset Shape:  (9357, 12)\n",
            "Outlier in the Dataset Shape:  (8577, 12)\n",
            "Without Outlier in the Dataset Shape:  (780, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using IQR Score\n"
      ],
      "metadata": {
        "id": "cC6CvkfMue0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = X.quantile(0.25)\n",
        "Q3 = X.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "#print(IQR)\n",
        "\n",
        "\n",
        "X_out = X[~((X < (Q1 - 1.5 * IQR)) |(X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "X_in = X[((X < (Q1 - 1.5 * IQR)) |(X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "\n",
        "print(\"Original Dataset Shape: \",X.shape)\n",
        "print(\"Outlier in the Dataset Shape: \",X_out.shape)\n",
        "print(\"Without Outlier in the Dataset Shape: \",X_in.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX0-wwAyujjQ",
        "outputId": "957b5647-7af2-448a-eeff-6ba1def1a843"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset Shape:  (9357, 12)\n",
            "Outlier in the Dataset Shape:  (5463, 12)\n",
            "Without Outlier in the Dataset Shape:  (3894, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing features with low variance"
      ],
      "metadata": {
        "id": "1ge8dHbhu4hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
        "sel.fit_transform(X)\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "Kry9G3nOu43P",
        "outputId": "844e85a3-b507-40ef-c929-1e2448998e5c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-77ca881f-5e41-41bb-bdf5-4a05bbfe51a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.00</td>\n",
              "      <td>150</td>\n",
              "      <td>11.881723</td>\n",
              "      <td>1045.50</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.25</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.00</td>\n",
              "      <td>1267.50</td>\n",
              "      <td>13.60</td>\n",
              "      <td>48.875001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.25</td>\n",
              "      <td>112</td>\n",
              "      <td>9.397165</td>\n",
              "      <td>954.75</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1173.75</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1558.75</td>\n",
              "      <td>972.25</td>\n",
              "      <td>13.30</td>\n",
              "      <td>47.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.00</td>\n",
              "      <td>88</td>\n",
              "      <td>8.997817</td>\n",
              "      <td>939.25</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.00</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1554.50</td>\n",
              "      <td>1074.00</td>\n",
              "      <td>11.90</td>\n",
              "      <td>53.975000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1375.50</td>\n",
              "      <td>80</td>\n",
              "      <td>9.228796</td>\n",
              "      <td>948.25</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1583.75</td>\n",
              "      <td>1203.25</td>\n",
              "      <td>11.00</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.25</td>\n",
              "      <td>51</td>\n",
              "      <td>6.518224</td>\n",
              "      <td>835.50</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.00</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.00</td>\n",
              "      <td>1110.00</td>\n",
              "      <td>11.15</td>\n",
              "      <td>59.575001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77ca881f-5e41-41bb-bdf5-4a05bbfe51a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77ca881f-5e41-41bb-bdf5-4a05bbfe51a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77ca881f-5e41-41bb-bdf5-4a05bbfe51a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   CO(GT)  PT08.S1(CO)  NMHC(GT)  ...  PT08.S5(O3)      T         RH\n",
              "0     2.6      1360.00       150  ...      1267.50  13.60  48.875001\n",
              "1     2.0      1292.25       112  ...       972.25  13.30  47.700000\n",
              "2     2.2      1402.00        88  ...      1074.00  11.90  53.975000\n",
              "3     2.2      1375.50        80  ...      1203.25  11.00  60.000000\n",
              "4     1.6      1272.25        51  ...      1110.00  11.15  59.575001\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAMLo_vnu9lP",
        "outputId": "bef0d8b6-6c6a-4054-be2f-7b69a26403aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9357 entries, 0 to 9356\n",
            "Data columns (total 12 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   CO(GT)         9357 non-null   float64\n",
            " 1   PT08.S1(CO)    9357 non-null   float64\n",
            " 2   NMHC(GT)       9357 non-null   int64  \n",
            " 3   C6H6(GT)       9357 non-null   float64\n",
            " 4   PT08.S2(NMHC)  9357 non-null   float64\n",
            " 5   NOx(GT)        9357 non-null   float64\n",
            " 6   PT08.S3(NOx)   9357 non-null   float64\n",
            " 7   NO2(GT)        9357 non-null   float64\n",
            " 8   PT08.S4(NO2)   9357 non-null   float64\n",
            " 9   PT08.S5(O3)    9357 non-null   float64\n",
            " 10  T              9357 non-null   float64\n",
            " 11  RH             9357 non-null   float64\n",
            "dtypes: float64(11), int64(1)\n",
            "memory usage: 877.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normality test"
      ],
      "metadata": {
        "id": "GP4Xx7rS1f7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro"
      ],
      "metadata": {
        "id": "_wHAzsDo1g0G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Extracting the numerical variables from Independent variables\n",
        "num_features=[i for i in X.columns if X.dtypes[i]=='float64']\n",
        "num_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WaL2eAg1sUv",
        "outputId": "1ac34f23-742e-47ce-e547-cb141e6368fe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CO(GT)',\n",
              " 'PT08.S1(CO)',\n",
              " 'C6H6(GT)',\n",
              " 'PT08.S2(NMHC)',\n",
              " 'NOx(GT)',\n",
              " 'PT08.S3(NOx)',\n",
              " 'NO2(GT)',\n",
              " 'PT08.S4(NO2)',\n",
              " 'PT08.S5(O3)',\n",
              " 'T',\n",
              " 'RH']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro\n",
        "\n",
        "for i in num_features:\n",
        "  print(\"============================================\") \n",
        "  print(\"Variable Name: \",i)\n",
        "  data = X[i]\n",
        "  stat, p = shapiro(data)\n",
        "  print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "  # interpret\n",
        "  alpha = 0.05\n",
        "  if p > alpha:\n",
        "\t   print('Sample looks Gaussian (fail to reject H0)')\n",
        "  else:\n",
        "\t   print('Sample does not look Gaussian (reject H0)')\n",
        "      \n",
        "  print(\"============================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lXlBrXt1ncF",
        "outputId": "316f122b-8b48-4cbe-a4eb-5363dd2b032a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================\n",
            "Variable Name:  CO(GT)\n",
            "Statistics=0.482, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  PT08.S1(CO)\n",
            "Statistics=0.823, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  C6H6(GT)\n",
            "Statistics=0.339, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  PT08.S2(NMHC)\n",
            "Statistics=0.931, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  NOx(GT)\n",
            "Statistics=0.924, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  PT08.S3(NOx)\n",
            "Statistics=0.925, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  NO2(GT)\n",
            "Statistics=0.777, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  PT08.S4(NO2)\n",
            "Statistics=0.902, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  PT08.S5(O3)\n",
            "Statistics=0.979, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  T\n",
            "Statistics=0.369, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n",
            "============================================\n",
            "Variable Name:  RH\n",
            "Statistics=0.500, p=0.000\n",
            "Sample does not look Gaussian (reject H0)\n",
            "============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXrKcLPc3HZL"
      },
      "source": [
        "#Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEmNqp-7Aufj",
        "outputId": "3a5f91fe-ace4-4101-c661-040596ffa56e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7485, 12) (1872, 12) (7485,) (1872,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get score using original model"
      ],
      "metadata": {
        "id": "qBfSSPogvRon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression()\n",
        "\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "scores = cross_val_score(reg, X_train, y_train, cv=10)\n",
        "print('CV accuracy (original): %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n",
        "highest_score = np.mean(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BYk_6JZvMJP",
        "outputId": "b86cd300-ee0d-4ec2-bc64-6eb76117ff1e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV accuracy (original): 0.999 +/- 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0i85RRHzsrs"
      },
      "source": [
        "#Ordinary Least Squares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk5HXwLjzOpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e5d1d5-c858-4e44-88ed-ee0dab639d0e"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "model_train = sm.OLS(y_train, X_train)\n",
        "results_train = model_train.fit()\n",
        "print(results_train.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                     AH   R-squared (uncentered):                   0.999\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.999\n",
            "Method:                 Least Squares   F-statistic:                          8.084e+05\n",
            "Date:                Sun, 23 Jan 2022   Prob (F-statistic):                        0.00\n",
            "Time:                        07:05:09   Log-Likelihood:                         -11335.\n",
            "No. Observations:                7485   AIC:                                  2.269e+04\n",
            "Df Residuals:                    7473   BIC:                                  2.278e+04\n",
            "Df Model:                          12                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "CO(GT)           -0.0004      0.000     -1.695      0.090      -0.001    5.91e-05\n",
            "PT08.S1(CO)       0.0020      0.000     12.540      0.000       0.002       0.002\n",
            "NMHC(GT)          0.0008      0.000      7.356      0.000       0.001       0.001\n",
            "C6H6(GT)          0.6213      0.003    205.368      0.000       0.615       0.627\n",
            "PT08.S2(NMHC)    -0.0106      0.000    -49.271      0.000      -0.011      -0.010\n",
            "NOx(GT)          -0.0043      0.000    -35.193      0.000      -0.005      -0.004\n",
            "PT08.S3(NOx)      0.0023   5.56e-05     40.754      0.000       0.002       0.002\n",
            "NO2(GT)           0.0070      0.000     32.336      0.000       0.007       0.007\n",
            "PT08.S4(NO2)     -0.0068   9.33e-05    -73.113      0.000      -0.007      -0.007\n",
            "PT08.S5(O3)       0.0007   9.39e-05      7.506      0.000       0.001       0.001\n",
            "T                 0.2848      0.002    120.684      0.000       0.280       0.289\n",
            "RH                0.1020      0.001    100.167      0.000       0.100       0.104\n",
            "==============================================================================\n",
            "Omnibus:                     2427.283   Durbin-Watson:                   2.012\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            21821.303\n",
            "Skew:                          -1.293   Prob(JB):                         0.00\n",
            "Kurtosis:                      10.955   Cond. No.                         740.\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDH24TI32wK7"
      },
      "source": [
        "#Linear Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lars\n",
        "from sklearn.linear_model import SGDRegressor"
      ],
      "metadata": {
        "id": "xODSkuTTwXfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9hwi5UF22WC"
      },
      "source": [
        "linear_Models = [LinearRegression(),ElasticNet(),Lasso(),Ridge(),Lars(n_nonzero_coefs=1),SGDRegressor(max_iter=1000, tol=1e-3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0vINm9UEGEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5275d101-20d3-4ed4-a9d3-a1da507a73b5"
      },
      "source": [
        "for mod in range(len(linear_Models)):\n",
        "  model = linear_Models[mod]\n",
        "  #print(model)\n",
        "  \n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  print(\"Metrics on Train samples\")\n",
        "  print(\"==============================================================\")\n",
        "  print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "  print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "  print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "  print(\"==============================================================\")\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Metrics on Test samples\")\n",
        "  print(\"==============================================================\")\n",
        "  print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "  print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "  print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "  print(\"==============================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999333566990804\n",
            "Multiple Regression mean_squared_error:  1.016381407363815\n",
            "Multiple Regression mean_absolute_error:  0.7430699504333376\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993159428345\n",
            "Multiple Regression mean_squared_error:  1.0224287679030157\n",
            "Multiple Regression mean_absolute_error:  0.7439265124057546\n",
            "==============================================================\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999333566990804\n",
            "Multiple Regression mean_squared_error:  1.016381407363815\n",
            "Multiple Regression mean_absolute_error:  0.7430699504333376\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993141838660017\n",
            "Multiple Regression mean_squared_error:  1.0250578171773583\n",
            "Multiple Regression mean_absolute_error:  0.7614329978675431\n",
            "==============================================================\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999333566990804\n",
            "Multiple Regression mean_squared_error:  1.016381407363815\n",
            "Multiple Regression mean_absolute_error:  0.7430699504333376\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993151740523404\n",
            "Multiple Regression mean_squared_error:  1.0235778312208665\n",
            "Multiple Regression mean_absolute_error:  0.7540866812144937\n",
            "==============================================================\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999333566990804\n",
            "Multiple Regression mean_squared_error:  1.016381407363815\n",
            "Multiple Regression mean_absolute_error:  0.7430699504333376\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993159471989222\n",
            "Multiple Regression mean_squared_error:  1.0224222446019158\n",
            "Multiple Regression mean_absolute_error:  0.7439277823628299\n",
            "==============================================================\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999333566990804\n",
            "Multiple Regression mean_squared_error:  1.016381407363815\n",
            "Multiple Regression mean_absolute_error:  0.7430699504333376\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.23066413290370424\n",
            "Multiple Regression mean_squared_error:  1149.8909190197794\n",
            "Multiple Regression mean_absolute_error:  13.16695940957751\n",
            "==============================================================\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999333566990804\n",
            "Multiple Regression mean_squared_error:  1.016381407363815\n",
            "Multiple Regression mean_absolute_error:  0.7430699504333376\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  -8.199149820414154e+26\n",
            "Multiple Regression mean_squared_error:  1.2254891946948309e+30\n",
            "Multiple Regression mean_absolute_error:  1057962939499324.0\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWsjVPBwzxo6"
      },
      "source": [
        "#Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk2DPGbgz1gj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d2befa-e391-4825-f372-358d31864f00"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "degree = 1\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        " \n",
        "# transform the features to higher degree features.\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.fit_transform(X_test)\n",
        "  \n",
        "print(X_train_poly.shape,y_train.shape)  \n",
        "# fit the transformed features to Linear Regression\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train_poly, y_train)\n",
        "\n",
        "y_train_pred = poly_model.predict(X_train_poly)\n",
        "\n",
        "print(\"Metrics on Train samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "print(\"==============================================================\")\n",
        "\n",
        "y_test_pred = poly_model.predict(X_test_poly)\n",
        "\n",
        "print(\"Metrics on Test samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "print(\"==============================================================\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7485, 13) (7485,)\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999333566990804\n",
            "Multiple Regression mean_squared_error:  1.016381407363815\n",
            "Multiple Regression mean_absolute_error:  0.7430699504333376\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993159428345\n",
            "Multiple Regression mean_squared_error:  1.0224287679030255\n",
            "Multiple Regression mean_absolute_error:  0.7439265124057473\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00pVPR8s28hV"
      },
      "source": [
        "#Algorithm Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "E9RxLbBWwg8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LatRFeA29Hj"
      },
      "source": [
        "Alg_Model_list = [KNeighborsRegressor(),SVR(),DecisionTreeRegressor()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp687IFuHSHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2283a014-31f9-48c8-f3ea-9452556f2922"
      },
      "source": [
        "for mod in range(len(Alg_Model_list)):\n",
        "  model = Alg_Model_list[mod]\n",
        "  print(model)\n",
        "  \n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  print(\"Model Used : \",model)\n",
        "  print(\"Metrics on Train samples\")\n",
        "  print(\"==============================================================\")\n",
        "  print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "  print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "  print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "  print(\"==============================================================\")\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Metrics on Test samples\")\n",
        "  print(\"==============================================================\")\n",
        "  print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "  print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "  print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "  print(\"==============================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNeighborsRegressor()\n",
            "Model Used :  KNeighborsRegressor()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999934864134153\n",
            "Multiple Regression mean_squared_error:  0.009933914149774728\n",
            "Multiple Regression mean_absolute_error:  0.06740271457375605\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999903252374572\n",
            "Multiple Regression mean_squared_error:  0.014460422381706164\n",
            "Multiple Regression mean_absolute_error:  0.083231898328982\n",
            "==============================================================\n",
            "SVR()\n",
            "Model Used :  SVR()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9981733717342091\n",
            "Multiple Regression mean_squared_error:  2.785802896760492\n",
            "Multiple Regression mean_absolute_error:  0.6340982613763587\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9980267608753377\n",
            "Multiple Regression mean_squared_error:  2.949309719651656\n",
            "Multiple Regression mean_absolute_error:  0.654877142105676\n",
            "==============================================================\n",
            "DecisionTreeRegressor()\n",
            "Model Used :  DecisionTreeRegressor()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  1.0\n",
            "Multiple Regression mean_squared_error:  3.280216182259494e-19\n",
            "Multiple Regression mean_absolute_error:  2.1811523595944438e-11\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999994144795554\n",
            "Multiple Regression mean_squared_error:  0.0008751504654201243\n",
            "Multiple Regression mean_absolute_error:  0.019886563107819\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVR"
      ],
      "metadata": {
        "id": "hxXv-RyP3jL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_validate"
      ],
      "metadata": {
        "id": "1LL2sdmzfUYB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsc = GridSearchCV(\n",
        "        estimator=SVR(kernel='rbf'),\n",
        "        param_grid={\n",
        "            'C': [0.01,0.1, 1, 10],\n",
        "            'epsilon': [0.01, 0.1, 1, 10],\n",
        "            'gamma': [0.01, 0.1, 1, 10]\n",
        "        },\n",
        "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)"
      ],
      "metadata": {
        "id": "CMcgTNKDPUEu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_result = gsc.fit(X, y)    "
      ],
      "metadata": {
        "id": "ScpqUY6DPZJl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = grid_result.best_params_\n",
        "best_svr = SVR(kernel='rbf', C=best_params[\"C\"], epsilon=best_params[\"epsilon\"], gamma=best_params[\"gamma\"],\n",
        "  coef0=0.1, shrinking=True,tol=0.001, cache_size=200, verbose=False, max_iter=-1)"
      ],
      "metadata": {
        "id": "MEtja5QGPfvd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scoring = {    'abs_error': 'neg_mean_absolute_error',\n",
        "               'squared_error': 'neg_mean_squared_error'}"
      ],
      "metadata": {
        "id": "GjxMLWGrPrKV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_validate(best_svr, X, y, cv=10, scoring=scoring, return_train_score=True)"
      ],
      "metadata": {
        "id": "wsF04yaEPufV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "print(\"MAE :\", abs(scores['test_abs_error'].mean()), \"| RMSE :\", math.sqrt(abs(scores['test_squared_error'].mean())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25uw-SiqP32l",
        "outputId": "aaadc208-945a-4f5a-8b4d-9d5451de5667"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE : 16.386009630652808 | RMSE : 35.228231072761716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree"
      ],
      "metadata": {
        "id": "CJ_fa5gJI7_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor \n",
        "from sklearn.tree import DecisionTreeRegressor  \n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "  \n",
        "# create a regressor object \n",
        "regressor = DecisionTreeRegressor(random_state = 0)  \n",
        "  \n",
        "# fit the regressor with X and Y data \n",
        "regressor.fit(X_train, y_train) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziN47rnUI-Zf",
        "outputId": "a28b6149-23be-44d8-d7f5-0869afa25a46"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = regressor.predict(X_train)\n",
        "\n",
        "print(\"Metrics on Train samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "print(\"==============================================================\")\n",
        "\n",
        "y_test_pred = regressor.predict(X_test)\n",
        "\n",
        "print(\"Metrics on Test samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "print(\"==============================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1JgXSJCJOfn",
        "outputId": "f09a8ce9-ee3e-4106-dec1-1016e4a1e105"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  1.0\n",
            "Multiple Regression mean_squared_error:  3.280216182259494e-19\n",
            "Multiple Regression mean_absolute_error:  2.1811523595944438e-11\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999999444259512\n",
            "Multiple Regression mean_squared_error:  0.0008306397345534152\n",
            "Multiple Regression mean_absolute_error:  0.01964848519873446\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "zywMFs5q3vAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "_ruWdLHV3vTk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rfr_model(X, y):\n",
        "# Perform Grid-Search\n",
        "    gsc = GridSearchCV(\n",
        "        estimator=RandomForestRegressor(),\n",
        "        param_grid={\n",
        "            'max_depth': range(3,7),\n",
        "            'n_estimators': (10, 50, 100, 1000),\n",
        "        },\n",
        "        cv=5, scoring='neg_mean_squared_error', verbose=0,                         n_jobs=-1)\n",
        "    \n",
        "    grid_result = gsc.fit(X, y)\n",
        "    best_params = grid_result.best_params_\n",
        "    \n",
        "    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],                               random_state=False, verbose=False)\n",
        "    # Perform K-Fold CV\n",
        "    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "4RRHgBPI34md"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfr = rfr_model(X,y)"
      ],
      "metadata": {
        "id": "AMXhq_mQ37Jl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y-eiEbL39W8",
        "outputId": "90f4a818-6874-4455-d7c7-ef24e625c037"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.05635059, -0.07816931, -0.07661456, -0.15531289, -0.08733195,\n",
              "       -0.08068411, -0.05425668, -0.0326124 , -0.04629723, -0.06503148])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa9Wvu2B3Cw0"
      },
      "source": [
        "#Ensemble Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import  XGBRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor"
      ],
      "metadata": {
        "id": "wW273ELxwpBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE2pCzVU3E7E"
      },
      "source": [
        "ensemble_Model_list = [RandomForestRegressor(),BaggingRegressor(),GradientBoostingRegressor(),XGBRegressor(),AdaBoostRegressor(),ExtraTreesRegressor()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpM7l_ZKHsww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda10444-1b66-4ffe-a6af-c23e15b065c8"
      },
      "source": [
        "for mod in range(len(ensemble_Model_list)):\n",
        "  model = ensemble_Model_list[mod]\n",
        "  print(model)\n",
        "  \n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  \n",
        "  print(\"Metrics on Train samples\")\n",
        "  print(\"==============================================================\")\n",
        "  print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "  print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "  print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "  print(\"==============================================================\")\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "\n",
        "  print(\"Metrics on Test samples\")\n",
        "  print(\"==============================================================\")\n",
        "  print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "  print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "  print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "  print(\"==============================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999999789863652\n",
            "Multiple Regression mean_squared_error:  3.204803399076439e-05\n",
            "Multiple Regression mean_absolute_error:  0.0031605904174225974\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999998779097274\n",
            "Multiple Regression mean_squared_error:  0.00018248271231167566\n",
            "Multiple Regression mean_absolute_error:  0.007846201662707507\n",
            "==============================================================\n",
            "BaggingRegressor()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999999556375258\n",
            "Multiple Regression mean_squared_error:  6.765750404952139e-05\n",
            "Multiple Regression mean_absolute_error:  0.0051670255396202265\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999998093266028\n",
            "Multiple Regression mean_squared_error:  0.0002849907527564856\n",
            "Multiple Regression mean_absolute_error:  0.010843232610533143\n",
            "==============================================================\n",
            "GradientBoostingRegressor()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.99999836616298\n",
            "Multiple Regression mean_squared_error:  0.002491775687686055\n",
            "Multiple Regression mean_absolute_error:  0.03706748814984669\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999980352504977\n",
            "Multiple Regression mean_squared_error:  0.002936620671779523\n",
            "Multiple Regression mean_absolute_error:  0.0404733908039819\n",
            "==============================================================\n",
            "XGBRegressor()\n",
            "[07:08:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999983770318178\n",
            "Multiple Regression mean_squared_error:  0.002475199551013564\n",
            "Multiple Regression mean_absolute_error:  0.03688156814300601\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999980415400714\n",
            "Multiple Regression mean_squared_error:  0.0029272199353425087\n",
            "Multiple Regression mean_absolute_error:  0.04038163514211476\n",
            "==============================================================\n",
            "AdaBoostRegressor()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.999980443566702\n",
            "Multiple Regression mean_squared_error:  0.02982564627520083\n",
            "Multiple Regression mean_absolute_error:  0.14166156807690944\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999793904247427\n",
            "Multiple Regression mean_squared_error:  0.030804183773024695\n",
            "Multiple Regression mean_absolute_error:  0.1427652810939151\n",
            "==============================================================\n",
            "ExtraTreesRegressor()\n",
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  1.0\n",
            "Multiple Regression mean_squared_error:  3.0542271995333163e-20\n",
            "Multiple Regression mean_absolute_error:  7.831201496674061e-12\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9999997321797528\n",
            "Multiple Regression mean_squared_error:  0.00040029859950941633\n",
            "Multiple Regression mean_absolute_error:  0.01247323018580557\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lasso Regression"
      ],
      "metadata": {
        "id": "0spOhy8wiNEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "std = StandardScaler()\n"
      ],
      "metadata": {
        "id": "ITdzNWsqhpal"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "# Setting up alpha list and letting the model select the best alpha  \n",
        "alphalist = 10**(np.linspace(-2,2,200))\n",
        "lasso_model = LassoCV(alphas = alphalist, cv=5) # setting K-fold \n",
        "lasso_model.fit(std.fit_transform(X_train), y_train)\n",
        "# printing coefficients \n",
        "list(zip(X_train.columns, lasso_model.coef_ / std.scale_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DFeaS4_h4-V",
        "outputId": "4dcdddd0-96f9-4d8d-c0dc-26e3b8c843a1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('CO(GT)', 0.0),\n",
              " ('PT08.S1(CO)', -0.0),\n",
              " ('NMHC(GT)', 0.0014724300030403942),\n",
              " ('C6H6(GT)', 0.7574252834900745),\n",
              " ('PT08.S2(NMHC)', -0.01655757153703197),\n",
              " ('NOx(GT)', -0.0031333995406014406),\n",
              " ('PT08.S3(NOx)', -0.0003933213320347902),\n",
              " ('NO2(GT)', 0.004996069578279553),\n",
              " ('PT08.S4(NO2)', -0.005027600802297846),\n",
              " ('PT08.S5(O3)', 0.00038497765095509874),\n",
              " ('T', 0.22714004132680005),\n",
              " ('RH', 0.07628797320937977)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ridge Regression"
      ],
      "metadata": {
        "id": "wi93FpytifFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "# Fit and train model\n",
        "alphalist = 10**(np.linspace(-1,2,200))\n",
        "ridge_model = RidgeCV(alphas = alphalist, cv=5)\n",
        "std.fit(X_train)\n",
        "ridge_model.fit(std.transform(X_train), y_train)\n",
        "# print coefficients\n",
        "list(zip(X_train.columns, ridge_model.coef_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvhkeAA-ifU8",
        "outputId": "4e5c62a4-981a-46dd-d7b9-d153c86e6c97"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('CO(GT)', -0.007269144474914222),\n",
              " ('PT08.S1(CO)', -0.199770269158771),\n",
              " ('NMHC(GT)', 0.2434718680542107),\n",
              " ('C6H6(GT)', 32.19620963769261),\n",
              " ('PT08.S2(NMHC)', -6.130883331158982),\n",
              " ('NOx(GT)', -0.8506930768757546),\n",
              " ('PT08.S3(NOx)', -0.3391381381489632),\n",
              " ('NO2(GT)', 0.6748469378536904),\n",
              " ('PT08.S4(NO2)', -2.258052853862863),\n",
              " ('PT08.S5(O3)', 0.35261140953689457),\n",
              " ('T', 9.63246230137085),\n",
              " ('RH', 3.7512185508637654)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ElasticNet"
      ],
      "metadata": {
        "id": "XXipW7qPZghi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lm_elastic = ElasticNet()\n",
        "lm_elastic.fit(X_train,y_train)\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "ft_importances_lm = pd.Series(lm_elastic.coef_,index = X.columns)\n",
        "ft_importances_lm.plot(kind='barh')\n",
        "plt.show()\n",
        "\n",
        "print(\"RSqure value for Elastic Net Regression Test Data is - \")\n",
        "np.round(lm_elastic.score(X_test,y_test)*100,2)\n",
        "\n",
        "predict_test_elastic = lm_elastic.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"ElasticNet Mean Square Error for Test Data is \")\n",
        "np.round(metrics.mean_squared_error(y_test,predict_test_elastic),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "8SQDCexHZgvy",
        "outputId": "1e476f4e-30c2-4188-cd83-a4e6529eddb1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RSqure value for Elastic Net Regression Test Data is - \n",
            "ElasticNet Mean Square Error for Test Data is \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.03"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Least-Angle Regression(LAR)"
      ],
      "metadata": {
        "id": "rFXTY1N4jOCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.Lars(n_nonzero_coefs=12)\n",
        "\n",
        "reg.fit(X,y)\n",
        "\n",
        "y_train_pred = reg.predict(X_train)\n",
        "\n",
        "print(\"Metrics on Train samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "print(\"==============================================================\")\n",
        "\n",
        "y_test_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"Metrics on Test samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "print(\"==============================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99zTeTNEjPAT",
        "outputId": "b4573b4d-0025-483b-89a1-b434cf34a96c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993130770968721\n",
            "Multiple Regression mean_squared_error:  1.0476306806498246\n",
            "Multiple Regression mean_absolute_error:  0.7546644848181379\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9992887909791771\n",
            "Multiple Regression mean_squared_error:  1.0630113966425658\n",
            "Multiple Regression mean_absolute_error:  0.7587304124472873\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stochastic Gradient Regression"
      ],
      "metadata": {
        "id": "JxjheHyVk73U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "sgd = SGDRegressor(max_iter=1, tol=1e-1)\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_train_pred = sgd.predict(X_train)\n",
        "\n",
        "print(\"Metrics on Train samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "print(\"==============================================================\")\n",
        "\n",
        "y_test_pred = sgd.predict(X_test)\n",
        "\n",
        "print(\"Metrics on Test samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "print(\"==============================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfOHZqlik-A0",
        "outputId": "e714d85e-41dd-4b46-f2cb-bc68e2727d52"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  -2.6348521083075016e+28\n",
            "Multiple Regression mean_squared_error:  4.0184304454964514e+31\n",
            "Multiple Regression mean_absolute_error:  6176777135238595.0\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  -2.690674828348161e+28\n",
            "Multiple Regression mean_squared_error:  4.0216278526442184e+31\n",
            "Multiple Regression mean_absolute_error:  6183573290871392.0\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLPRegressor"
      ],
      "metadata": {
        "id": "11yAyhXBlmS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "model = MLPRegressor()\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "print(\"Metrics on Train samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "print(\"==============================================================\")\n",
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Metrics on Test samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "print(\"==============================================================\")\n",
        "print(\"==============================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_S8ybhUlmhD",
        "outputId": "ba579ca0-7b0f-4158-8fc2-2119afa99142"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993041449453784\n",
            "Multiple Regression mean_squared_error:  1.0612531642013097\n",
            "Multiple Regression mean_absolute_error:  0.7412220739799221\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  0.9993037198824903\n",
            "Multiple Regression mean_squared_error:  1.0406978518241778\n",
            "Multiple Regression mean_absolute_error:  0.7268044297782833\n",
            "==============================================================\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA & Grid Search"
      ],
      "metadata": {
        "id": "3w5x61CtzoEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import decomposition\n",
        "\n",
        "# Plot PCA spectrum\n",
        "pca = decomposition.PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "plt.figure(1, figsize=(20, 10))\n",
        "plt.plot(pca.explained_variance_, linewidth=2)\n",
        "plt.axis('tight')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Explained variance')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "cwy40CrHzoXr",
        "outputId": "568ec47f-4b15-4720-9a67-d0a312719da0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Explained variance')"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "etr = ExtraTreesRegressor()\n",
        "\n",
        "pipeline = Pipeline(steps=[('pca', pca), \n",
        "                           ('etr', etr)])\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(pipeline.get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ir8u-k1zz-W",
        "outputId": "287f848b-bba7-4406-8e73-d4941078f889"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'memory': None, 'steps': [('pca', PCA()), ('etr', ExtraTreesRegressor())], 'verbose': False, 'pca': PCA(), 'etr': ExtraTreesRegressor(), 'pca__copy': True, 'pca__iterated_power': 'auto', 'pca__n_components': None, 'pca__random_state': None, 'pca__svd_solver': 'auto', 'pca__tol': 0.0, 'pca__whiten': False, 'etr__bootstrap': False, 'etr__ccp_alpha': 0.0, 'etr__criterion': 'squared_error', 'etr__max_depth': None, 'etr__max_features': 'auto', 'etr__max_leaf_nodes': None, 'etr__max_samples': None, 'etr__min_impurity_decrease': 0.0, 'etr__min_samples_leaf': 1, 'etr__min_samples_split': 2, 'etr__min_weight_fraction_leaf': 0.0, 'etr__n_estimators': 100, 'etr__n_jobs': None, 'etr__oob_score': False, 'etr__random_state': None, 'etr__verbose': 0, 'etr__warm_start': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {'etr__n_estimators' : [10,100,500],\n",
        "                   'etr__max_depth' : [None, 20, 50],\n",
        "                   'pca__n_components' : [9,19,39]}\n",
        "\n",
        "clf = GridSearchCV(pipeline,\n",
        "                   hyperparameters,\n",
        "                   cv=10)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Best params\n",
        "print(clf.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEq8nQ1Qz624",
        "outputId": "da144853-659d-4611-f999-97afe98a852c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'etr__max_depth': None, 'etr__n_estimators': 10, 'pca__n_components': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Plot the residuals\n",
        "residuals = y_test-y_pred\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test, residuals)\n",
        "ax.axhline(lw=2,color='black')\n",
        "ax.set_xlabel('Observed')\n",
        "ax.set_ylabel('Residuals')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "BEnUI15v0Af2",
        "outputId": "e86d3106-4aba-42ed-b40f-46461aa1fda7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA Regression"
      ],
      "metadata": {
        "id": "uAAkLUJCspKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import scale \n",
        "from sklearn import model_selection\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "BmY21lAnpfdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale "
      ],
      "metadata": {
        "id": "wj70TXdrqich"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "X_reduced = pca.fit_transform(scale(X))"
      ],
      "metadata": {
        "id": "nQuvs0BJr0VZ"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10-fold CV, with shuffle\n",
        "n = len(X_reduced)\n",
        "#n = 12\n",
        "kf_10 = KFold(n, shuffle=True, random_state=1)\n",
        "\n",
        "regr = LinearRegression()\n",
        "mse = []\n",
        "\n",
        "# Calculate MSE with only the intercept (no principal components in regression)\n",
        "score = -1*cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
        "mse.append(score)\n",
        "\n",
        "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
        "for i in np.arange(1, 20):\n",
        "    score = -1*cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
        "    mse.append(score)\n",
        "    \n",
        "# Plot results    \n",
        "plt.plot(mse, '-v')\n",
        "plt.xlabel('Number of principal components in regression')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Air Quality')\n",
        "plt.xlim(xmin=-1);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Ab7Rn72Mp93a",
        "outputId": "6039b69c-a0b9-4d36-b5e8-05d282ee5c4e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLsJlFna3D4G",
        "outputId": "4fd7df0b-c868-43ce-c584-1a629e652f78"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 47.91,  71.79,  83.01,  91.56,  94.83,  96.92,  97.96,  98.83,\n",
              "        99.44,  99.76,  99.99, 100.01])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca2 = PCA()\n",
        "\n",
        "# Split into training and test sets\n",
        "#X_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)\n",
        "\n",
        "# Scale the data\n",
        "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
        "n = len(X_reduced_train)\n",
        "\n",
        "# 10-fold CV, with shuffle\n",
        "kf_10 = KFold(n, shuffle=True, random_state=1)\n",
        "\n",
        "mse = []\n",
        "\n",
        "# Calculate MSE with only the intercept (no principal components in regression)\n",
        "score = -1*cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
        "mse.append(score)\n",
        "\n",
        "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
        "for i in np.arange(1, 20):\n",
        "    score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
        "    mse.append(score)\n",
        "\n",
        "plt.plot(np.array(mse), '-v')\n",
        "plt.xlabel('Number of principal components in regression')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Air Quality')\n",
        "plt.xlim(xmin=-1);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KVimqKoa3I3e",
        "outputId": "37b1bb93-6618-4bdf-ed04-551b8aa73e5f"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_reduced_test = pca2.transform(scale(X_test))[:,:7]\n",
        "\n",
        "# Train regression model on training data \n",
        "regr = LinearRegression()\n",
        "regr.fit(X_reduced_train[:,:7], y_train)\n",
        "\n",
        "# Prediction with test data\n",
        "pred = regr.predict(X_reduced_test)\n",
        "mean_squared_error(y_test, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7JAd1Or3ga1",
        "outputId": "6d8a4985-453a-45ec-ff1c-9d7e4b222589"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.925551835261086"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Partial Least Squares"
      ],
      "metadata": {
        "id": "W9QC74Yi3lB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "n = len(X_train)\n",
        "\n",
        "# 10-fold CV, with shuffle\n",
        "kf_10 = KFold(n, shuffle=True, random_state=1)\n",
        "\n",
        "mse = []\n",
        "\n",
        "for i in np.arange(1, 20):\n",
        "    pls = PLSRegression(n_components=i)\n",
        "    score = cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
        "    mse.append(score)\n",
        "\n",
        "# Plot results\n",
        "plt.plot(np.arange(1, 20), np.array(mse), '-v')\n",
        "plt.xlabel('Number of principal components in regression')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Air Quality')\n",
        "plt.xlim(xmin=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "LgIdpIo73p0d",
        "outputId": "01a4fca4-6c83-4432-a79c-5079a5fd3cca"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.0, 19.9)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pls = PLSRegression(n_components=2)\n",
        "pls.fit(scale(X_train), y_train)\n",
        "\n",
        "mean_squared_error(y_test, pls.predict(scale(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCr-mIQj3txd",
        "outputId": "9548d5af-23e3-4a1d-f890-4931711bb76d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42.106096672467224"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Averaged base models class"
      ],
      "metadata": {
        "id": "wVWpYdDJl3iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "\n",
        "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "        \n",
        "    # we define clones of the original models to fit the data in\n",
        "    def fit(self, X, y):\n",
        "        self.models_ = [clone(x) for x in self.models]\n",
        "        \n",
        "        # Train cloned base models\n",
        "        for model in self.models_:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    #Now we do the predictions for cloned models and average them\n",
        "    def predict(self, X):\n",
        "        predictions = np.column_stack([\n",
        "            model.predict(X) for model in self.models_\n",
        "        ])\n",
        "        return np.mean(predictions, axis=1)   "
      ],
      "metadata": {
        "id": "o5e2kowCl4Ar"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyVtt-BSmVLr",
        "outputId": "076163e9-5613-4436-aed6-8413b90cf86b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "en = ElasticNet()\n",
        "\n",
        "en.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIPXIpgumZ0N",
        "outputId": "9c618b77-a69c-4b0d-e351-756dd890e6df"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElasticNet()"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "ls = Lasso()\n",
        "\n",
        "ls.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mElaGLosmeHj",
        "outputId": "5cee3b69-9be7-4020-9b0d-aa802e38bab9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso()"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "rd = Lasso()\n",
        "\n",
        "rd.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y63AXDEWmkST",
        "outputId": "483788b4-5825-4a24-bb31-c378635e8f4e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso()"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sgd = SGDRegressor(max_iter=1000, tol=1e-3)\n",
        "sgd.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-AFL56FmqID",
        "outputId": "870d058d-bf8e-470b-d228-095ca1f9056c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lars\n",
        "lar = Lars(n_nonzero_coefs=1)\n",
        "\n",
        "lar.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCSTQWlYmvKP",
        "outputId": "cdd5afa5-5ff7-482c-b5d8-8a2ccda671b5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lars(n_nonzero_coefs=1)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "kn = KNeighborsRegressor(n_neighbors=2)\n",
        "\n",
        "kn.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSYbbRT6m0er",
        "outputId": "2bf3f48c-c9c5-4f6a-e0cf-e2427f2b146a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsRegressor(n_neighbors=2)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "s = SVR()\n",
        "\n",
        "s.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69FKkfybm5zw",
        "outputId": "07372815-5636-438c-fa86-a62250850a5a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR()"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor()\n",
        "\n",
        "dt.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ul1UaNOm-qj",
        "outputId": "6c2fc7d0-8bff-4c26-d1c1-362f1d2c1681"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "rf.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9DvdjhLnCmj",
        "outputId": "1145757e-fa15-46f7-8c67-04cf7ccb942e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "bg = BaggingRegressor()\n",
        "\n",
        "bg.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njSZXIZPnH6j",
        "outputId": "70e33a80-5ac8-4f6b-9fe0-d9f414136d71"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "gbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL4ZDamYnMkj",
        "outputId": "3f2d7dda-0bcd-40bc-eb4c-b49ea483729e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import  XGBRegressor \n",
        "\n",
        "xbr = XGBRegressor()\n",
        "\n",
        "xbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiIcy0GYnRkq",
        "outputId": "17f4793b-76ee-4e06-a2e8-29ad1e8ee623"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15:45:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_models = AveragingModels(models = (lr, en, ls,rd,sgd,lar,kn,s,dt,rf,bg,gbr,xbr))\n",
        "print(averaged_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ido_8Ee9mBJ7",
        "outputId": "af5cedd6-42cb-4588-beec-ed6012c45dbe"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AveragingModels(models=(LinearRegression(), ElasticNet(), Lasso(), Lasso(),\n",
            "                        SGDRegressor(), Lars(n_nonzero_coefs=1),\n",
            "                        KNeighborsRegressor(n_neighbors=2), SVR(),\n",
            "                        DecisionTreeRegressor(), RandomForestRegressor(),\n",
            "                        BaggingRegressor(), GradientBoostingRegressor(),\n",
            "                        XGBRegressor()))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "averaged_models.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_rEsZjxna_S",
        "outputId": "fa00ea75-e4c9-42b4-8dc5-d6829b73ec39"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15:46:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AveragingModels(models=(LinearRegression(), ElasticNet(), Lasso(), Lasso(),\n",
              "                        SGDRegressor(), Lars(n_nonzero_coefs=1),\n",
              "                        KNeighborsRegressor(n_neighbors=2), SVR(),\n",
              "                        DecisionTreeRegressor(), RandomForestRegressor(),\n",
              "                        BaggingRegressor(), GradientBoostingRegressor(),\n",
              "                        XGBRegressor()))"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = averaged_models.predict(X_train)\n",
        "\n",
        "print(\"Metrics on Train samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_train,y_train_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_train,y_train_pred))\n",
        "print(\"==============================================================\")\n",
        "\n",
        "y_test_pred = averaged_models.predict(X_test)\n",
        "\n",
        "print(\"Metrics on Test samples\")\n",
        "print(\"==============================================================\")\n",
        "print(\"Multiple Regression score: \",r2_score(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_squared_error: \",mean_squared_error(y_test,y_test_pred))\n",
        "print(\"Multiple Regression mean_absolute_error: \",mean_absolute_error(y_test,y_test_pred))\n",
        "print(\"==============================================================\")\n",
        "print(\"==============================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmRmCzM-ngtz",
        "outputId": "66e17987-b183-4865-cc42-da3b6d5149f2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics on Train samples\n",
            "==============================================================\n",
            "Multiple Regression score:  -1.8398346569921318e+24\n",
            "Multiple Regression mean_squared_error:  2.8059440516704216e+27\n",
            "Multiple Regression mean_absolute_error:  46256915068848.234\n",
            "==============================================================\n",
            "Metrics on Test samples\n",
            "==============================================================\n",
            "Multiple Regression score:  -1.8402292101583806e+24\n",
            "Multiple Regression mean_squared_error:  2.7505059209870436e+27\n",
            "Multiple Regression mean_absolute_error:  45685315721299.26\n",
            "==============================================================\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keras Network"
      ],
      "metadata": {
        "id": "JO8vRBg8fUJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Regression from keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1,input_shape=(12,)))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(tf.keras.optimizers.Adam(lr=0.8),'mean_squared_error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSO9ew49fUaw",
        "outputId": "8819b47a-9412-421b-e32e-5bbf8730a358"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13\n",
            "Trainable params: 13\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "458xEXfnfbPO",
        "outputId": "ca71f51b-f21c-4677-cf7c-47993e30f142"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "293/293 [==============================] - 1s 1ms/step - loss: 249463.3906\n",
            "Epoch 2/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 61.7773\n",
            "Epoch 3/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 20.9229\n",
            "Epoch 4/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12.1679\n",
            "Epoch 5/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 85.9846\n",
            "Epoch 6/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 37999.5859\n",
            "Epoch 7/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 3276.3640\n",
            "Epoch 8/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 17715.9980\n",
            "Epoch 9/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8251.6182\n",
            "Epoch 10/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 37156.7539\n",
            "Epoch 11/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 18010.8516\n",
            "Epoch 12/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 64149.8203\n",
            "Epoch 13/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 458.1855\n",
            "Epoch 14/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 13352.8271\n",
            "Epoch 15/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 81729.9766\n",
            "Epoch 16/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 43322.4375\n",
            "Epoch 17/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 9.0691\n",
            "Epoch 18/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 76.6728\n",
            "Epoch 19/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 47162.7305\n",
            "Epoch 20/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 260.1255\n",
            "Epoch 21/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51976.4258\n",
            "Epoch 22/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10341.5098\n",
            "Epoch 23/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 110.9873\n",
            "Epoch 24/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 66174.4531\n",
            "Epoch 25/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12.7517\n",
            "Epoch 26/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1261.9360\n",
            "Epoch 27/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 48102.8945\n",
            "Epoch 28/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 166.1046\n",
            "Epoch 29/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 55371.1797\n",
            "Epoch 30/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 3031.6448\n",
            "Epoch 31/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12890.9541\n",
            "Epoch 32/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12451.7520\n",
            "Epoch 33/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51932.2773\n",
            "Epoch 34/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1012.8453\n",
            "Epoch 35/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 35876.2500\n",
            "Epoch 36/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 765.7333\n",
            "Epoch 37/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 31787.6211\n",
            "Epoch 38/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 76210.4609\n",
            "Epoch 39/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 18.7363\n",
            "Epoch 40/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 197.6424\n",
            "Epoch 41/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 36592.5781\n",
            "Epoch 42/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8469.8955\n",
            "Epoch 43/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28322.5723\n",
            "Epoch 44/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12681.6602\n",
            "Epoch 45/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 41001.1367\n",
            "Epoch 46/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 223.4572\n",
            "Epoch 47/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 22025.0664\n",
            "Epoch 48/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 19035.3848\n",
            "Epoch 49/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28530.2051\n",
            "Epoch 50/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 16379.0596\n",
            "Epoch 51/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 30074.3262\n",
            "Epoch 52/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 63934.0312\n",
            "Epoch 53/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 781.8449\n",
            "Epoch 54/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 136.5567\n",
            "Epoch 55/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 76128.1328\n",
            "Epoch 56/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 71.1289\n",
            "Epoch 57/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 3990.7356\n",
            "Epoch 58/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 56745.2969\n",
            "Epoch 59/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 100.5328\n",
            "Epoch 60/400\n",
            "293/293 [==============================] - 1s 2ms/step - loss: 40245.1914\n",
            "Epoch 61/400\n",
            "293/293 [==============================] - 1s 2ms/step - loss: 1962.2460\n",
            "Epoch 62/400\n",
            "293/293 [==============================] - 1s 2ms/step - loss: 35315.9805\n",
            "Epoch 63/400\n",
            "293/293 [==============================] - 0s 2ms/step - loss: 11262.0771\n",
            "Epoch 64/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 16777.2305\n",
            "Epoch 65/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 87823.0938\n",
            "Epoch 66/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 16.7734\n",
            "Epoch 67/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 96.6812\n",
            "Epoch 68/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 40149.9570\n",
            "Epoch 69/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 310.7466\n",
            "Epoch 70/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 141416.1094\n",
            "Epoch 71/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12.0901\n",
            "Epoch 72/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 13.9808\n",
            "Epoch 73/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 26.9193\n",
            "Epoch 74/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 31203.8926\n",
            "Epoch 75/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8328.1016\n",
            "Epoch 76/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 43732.2188\n",
            "Epoch 77/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 41300.6875\n",
            "Epoch 78/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8.7805\n",
            "Epoch 79/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 20032.7227\n",
            "Epoch 80/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 57528.4570\n",
            "Epoch 81/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 45.4146\n",
            "Epoch 82/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 5995.4590\n",
            "Epoch 83/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 67969.8281\n",
            "Epoch 84/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 973.8688\n",
            "Epoch 85/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51450.7500\n",
            "Epoch 86/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 29992.5156\n",
            "Epoch 87/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 44.2022\n",
            "Epoch 88/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 5495.6719\n",
            "Epoch 89/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51423.0000\n",
            "Epoch 90/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 33.9134\n",
            "Epoch 91/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 97663.4219\n",
            "Epoch 92/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 217.7804\n",
            "Epoch 93/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 21.9108\n",
            "Epoch 94/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 65947.4922\n",
            "Epoch 95/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 23661.5703\n",
            "Epoch 96/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 17.6101\n",
            "Epoch 97/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 17515.2812\n",
            "Epoch 98/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 4116.5029\n",
            "Epoch 99/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 40973.3867\n",
            "Epoch 100/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 15427.2314\n",
            "Epoch 101/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 35806.4609\n",
            "Epoch 102/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 4449.8726\n",
            "Epoch 103/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 37369.9141\n",
            "Epoch 104/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 2129.7024\n",
            "Epoch 105/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 57674.0625\n",
            "Epoch 106/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1673.3965\n",
            "Epoch 107/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 37609.3516\n",
            "Epoch 108/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 273.2019\n",
            "Epoch 109/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 52248.9414\n",
            "Epoch 110/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 68.0600\n",
            "Epoch 111/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 100900.7109\n",
            "Epoch 112/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 144.6119\n",
            "Epoch 113/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10.1376\n",
            "Epoch 114/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 14273.1611\n",
            "Epoch 115/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 45054.9219\n",
            "Epoch 116/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 205.8613\n",
            "Epoch 117/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 44164.4727\n",
            "Epoch 118/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 62.0457\n",
            "Epoch 119/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 95274.8047\n",
            "Epoch 120/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8.4105\n",
            "Epoch 121/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 21.1434\n",
            "Epoch 122/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 72318.8047\n",
            "Epoch 123/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 851.4443\n",
            "Epoch 124/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 153.3414\n",
            "Epoch 125/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 119347.8594\n",
            "Epoch 126/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 921.3915\n",
            "Epoch 127/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 7.0727\n",
            "Epoch 128/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 19.2144\n",
            "Epoch 129/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 42295.5039\n",
            "Epoch 130/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1874.3043\n",
            "Epoch 131/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 74791.5469\n",
            "Epoch 132/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 106.1602\n",
            "Epoch 133/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 75.7820\n",
            "Epoch 134/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 50669.7969\n",
            "Epoch 135/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 154.8732\n",
            "Epoch 136/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 79259.4297\n",
            "Epoch 137/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8548.6836\n",
            "Epoch 138/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6.8479\n",
            "Epoch 139/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 40879.7773\n",
            "Epoch 140/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 4084.5254\n",
            "Epoch 141/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 49608.6211\n",
            "Epoch 142/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6526.8960\n",
            "Epoch 143/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10405.7510\n",
            "Epoch 144/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 54783.1211\n",
            "Epoch 145/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 304.2365\n",
            "Epoch 146/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 62785.2773\n",
            "Epoch 147/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 91.0260\n",
            "Epoch 148/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6845.3291\n",
            "Epoch 149/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 38354.3047\n",
            "Epoch 150/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1706.4481\n",
            "Epoch 151/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 72025.7891\n",
            "Epoch 152/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 16.8008\n",
            "Epoch 153/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 44633.2695\n",
            "Epoch 154/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 20903.5957\n",
            "Epoch 155/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 27.1687\n",
            "Epoch 156/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 22899.4980\n",
            "Epoch 157/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 22808.0684\n",
            "Epoch 158/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 44435.7031\n",
            "Epoch 159/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 34.6750\n",
            "Epoch 160/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 77830.9453\n",
            "Epoch 161/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1360.7211\n",
            "Epoch 162/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11.7513\n",
            "Epoch 163/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 40043.0312\n",
            "Epoch 164/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1066.3066\n",
            "Epoch 165/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 25243.7051\n",
            "Epoch 166/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 71520.2188\n",
            "Epoch 167/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 54.7427\n",
            "Epoch 168/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 2092.9568\n",
            "Epoch 169/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 31435.1426\n",
            "Epoch 170/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12851.3936\n",
            "Epoch 171/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51135.8789\n",
            "Epoch 172/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 72.4565\n",
            "Epoch 173/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 89039.3281\n",
            "Epoch 174/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12.1580\n",
            "Epoch 175/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 38.3503\n",
            "Epoch 176/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 33301.2852\n",
            "Epoch 177/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 30059.5625\n",
            "Epoch 178/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 440.3510\n",
            "Epoch 179/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 21872.2090\n",
            "Epoch 180/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 150934.6250\n",
            "Epoch 181/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12985.9756\n",
            "Epoch 182/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 3.8669\n",
            "Epoch 183/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 4.7462\n",
            "Epoch 184/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 16.9710\n",
            "Epoch 185/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 63692.5859\n",
            "Epoch 186/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 108.3855\n",
            "Epoch 187/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 588.6271\n",
            "Epoch 188/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 64067.4609\n",
            "Epoch 189/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 46.2501\n",
            "Epoch 190/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 35556.3789\n",
            "Epoch 191/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10309.9023\n",
            "Epoch 192/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 30037.7891\n",
            "Epoch 193/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 29282.2715\n",
            "Epoch 194/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 210.8273\n",
            "Epoch 195/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 39352.6367\n",
            "Epoch 196/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1309.8701\n",
            "Epoch 197/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 125120.1016\n",
            "Epoch 198/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 4.9631\n",
            "Epoch 199/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6.1939\n",
            "Epoch 200/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 9236.7949\n",
            "Epoch 201/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 64496.1094\n",
            "Epoch 202/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10.7439\n",
            "Epoch 203/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 9051.0000\n",
            "Epoch 204/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 61425.7812\n",
            "Epoch 205/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 923.5380\n",
            "Epoch 206/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1895.5753\n",
            "Epoch 207/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 47362.0195\n",
            "Epoch 208/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6270.5435\n",
            "Epoch 209/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 43346.5664\n",
            "Epoch 210/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1042.3612\n",
            "Epoch 211/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 36651.2734\n",
            "Epoch 212/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 32125.1484\n",
            "Epoch 213/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1137.5862\n",
            "Epoch 214/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 24320.4355\n",
            "Epoch 215/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 31913.1660\n",
            "Epoch 216/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8670.7314\n",
            "Epoch 217/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 54977.4336\n",
            "Epoch 218/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 423.7025\n",
            "Epoch 219/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 5600.7256\n",
            "Epoch 220/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 26985.5664\n",
            "Epoch 221/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 20471.3359\n",
            "Epoch 222/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 29954.2852\n",
            "Epoch 223/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 53760.6055\n",
            "Epoch 224/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11.4659\n",
            "Epoch 225/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 41477.9414\n",
            "Epoch 226/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 5669.9424\n",
            "Epoch 227/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 29600.0312\n",
            "Epoch 228/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 18033.8770\n",
            "Epoch 229/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 9351.0527\n",
            "Epoch 230/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 32871.3125\n",
            "Epoch 231/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 5751.3057\n",
            "Epoch 232/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 38394.4062\n",
            "Epoch 233/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28046.8262\n",
            "Epoch 234/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1060.6193\n",
            "Epoch 235/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 79365.5625\n",
            "Epoch 236/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 40.6679\n",
            "Epoch 237/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 50.7409\n",
            "Epoch 238/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51356.3125\n",
            "Epoch 239/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 62.6382\n",
            "Epoch 240/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 126565.8203\n",
            "Epoch 241/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 4.2118\n",
            "Epoch 242/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11.6796\n",
            "Epoch 243/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28.2554\n",
            "Epoch 244/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 46344.9805\n",
            "Epoch 245/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 40317.6953\n",
            "Epoch 246/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 37.9228\n",
            "Epoch 247/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 47900.6758\n",
            "Epoch 248/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 86.6921\n",
            "Epoch 249/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 32494.1211\n",
            "Epoch 250/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 3090.4214\n",
            "Epoch 251/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 35215.2656\n",
            "Epoch 252/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 44396.5938\n",
            "Epoch 253/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 7825.8354\n",
            "Epoch 254/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 478.6582\n",
            "Epoch 255/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 98305.8438\n",
            "Epoch 256/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8.4757\n",
            "Epoch 257/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11.4769\n",
            "Epoch 258/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 17406.3574\n",
            "Epoch 259/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 19685.0195\n",
            "Epoch 260/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 85963.2188\n",
            "Epoch 261/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 27.8205\n",
            "Epoch 262/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 42.0262\n",
            "Epoch 263/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11523.1895\n",
            "Epoch 264/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 34935.6836\n",
            "Epoch 265/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 41523.2695\n",
            "Epoch 266/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 45.3658\n",
            "Epoch 267/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 47661.3242\n",
            "Epoch 268/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 50.2470\n",
            "Epoch 269/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 44470.5703\n",
            "Epoch 270/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 303.2221\n",
            "Epoch 271/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 40318.1328\n",
            "Epoch 272/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 49478.2812\n",
            "Epoch 273/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 3072.7769\n",
            "Epoch 274/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1709.6621\n",
            "Epoch 275/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 78371.6016\n",
            "Epoch 276/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6.4088\n",
            "Epoch 277/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 86.3801\n",
            "Epoch 278/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 66550.1406\n",
            "Epoch 279/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10.9662\n",
            "Epoch 280/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 25757.7383\n",
            "Epoch 281/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 25146.4160\n",
            "Epoch 282/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 5683.2822\n",
            "Epoch 283/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 47405.3047\n",
            "Epoch 284/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 318.7990\n",
            "Epoch 285/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 42501.7734\n",
            "Epoch 286/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11717.7109\n",
            "Epoch 287/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 46456.1602\n",
            "Epoch 288/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 63840.9062\n",
            "Epoch 289/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 12.0928\n",
            "Epoch 290/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 62.1488\n",
            "Epoch 291/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28474.1934\n",
            "Epoch 292/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 42385.1172\n",
            "Epoch 293/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 333.0299\n",
            "Epoch 294/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11951.8320\n",
            "Epoch 295/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 36741.8633\n",
            "Epoch 296/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 14480.3008\n",
            "Epoch 297/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 37983.2266\n",
            "Epoch 298/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 214.2585\n",
            "Epoch 299/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 132833.4062\n",
            "Epoch 300/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6.0764\n",
            "Epoch 301/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 23.6460\n",
            "Epoch 302/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 2660.0061\n",
            "Epoch 303/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28099.7070\n",
            "Epoch 304/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 44421.2227\n",
            "Epoch 305/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10365.0303\n",
            "Epoch 306/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8623.7920\n",
            "Epoch 307/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 27743.8574\n",
            "Epoch 308/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 18382.8809\n",
            "Epoch 309/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 31218.7891\n",
            "Epoch 310/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 21890.4238\n",
            "Epoch 311/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 14221.0684\n",
            "Epoch 312/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 89017.2188\n",
            "Epoch 313/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8528.0654\n",
            "Epoch 314/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 27.2374\n",
            "Epoch 315/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 3422.4031\n",
            "Epoch 316/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 34170.2031\n",
            "Epoch 317/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 56554.7227\n",
            "Epoch 318/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1424.2710\n",
            "Epoch 319/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 591.6703\n",
            "Epoch 320/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 61626.9883\n",
            "Epoch 321/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 22.9667\n",
            "Epoch 322/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10356.7627\n",
            "Epoch 323/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 29453.1367\n",
            "Epoch 324/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 36202.8906\n",
            "Epoch 325/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1084.3192\n",
            "Epoch 326/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 34316.1641\n",
            "Epoch 327/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 2339.1694\n",
            "Epoch 328/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 41906.3281\n",
            "Epoch 329/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 645.1469\n",
            "Epoch 330/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 31581.8496\n",
            "Epoch 331/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 37000.2539\n",
            "Epoch 332/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 534.1360\n",
            "Epoch 333/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 30018.2051\n",
            "Epoch 334/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 34537.4766\n",
            "Epoch 335/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 7459.4517\n",
            "Epoch 336/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 136755.5625\n",
            "Epoch 337/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28.0240\n",
            "Epoch 338/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6.3307\n",
            "Epoch 339/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 7.5240\n",
            "Epoch 340/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51663.3594\n",
            "Epoch 341/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 17948.9180\n",
            "Epoch 342/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 62.2767\n",
            "Epoch 343/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 99121.7969\n",
            "Epoch 344/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 81.5710\n",
            "Epoch 345/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8.7749\n",
            "Epoch 346/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 18199.3027\n",
            "Epoch 347/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 41691.2617\n",
            "Epoch 348/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 107.9775\n",
            "Epoch 349/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 24615.4941\n",
            "Epoch 350/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 51487.4258\n",
            "Epoch 351/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 204.1409\n",
            "Epoch 352/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 50584.8555\n",
            "Epoch 353/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 133.1426\n",
            "Epoch 354/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 81349.0469\n",
            "Epoch 355/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 725.8976\n",
            "Epoch 356/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 54.7160\n",
            "Epoch 357/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 49454.3086\n",
            "Epoch 358/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1731.9686\n",
            "Epoch 359/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8081.4150\n",
            "Epoch 360/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 68682.6484\n",
            "Epoch 361/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 10.7610\n",
            "Epoch 362/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 32346.5762\n",
            "Epoch 363/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 163142.7031\n",
            "Epoch 364/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 7.0877\n",
            "Epoch 365/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 4.5047\n",
            "Epoch 366/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 73.4450\n",
            "Epoch 367/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11674.9561\n",
            "Epoch 368/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 52630.5742\n",
            "Epoch 369/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 30.1954\n",
            "Epoch 370/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 60090.0156\n",
            "Epoch 371/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 70.3559\n",
            "Epoch 372/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 6865.3164\n",
            "Epoch 373/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28824.9277\n",
            "Epoch 374/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 55634.4766\n",
            "Epoch 375/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 80.7992\n",
            "Epoch 376/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 32718.0938\n",
            "Epoch 377/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 20518.9277\n",
            "Epoch 378/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 21933.2910\n",
            "Epoch 379/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 5421.6226\n",
            "Epoch 380/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 28662.2754\n",
            "Epoch 381/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 33519.5156\n",
            "Epoch 382/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 2546.0266\n",
            "Epoch 383/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 48385.3984\n",
            "Epoch 384/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 101.3075\n",
            "Epoch 385/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 47135.3555\n",
            "Epoch 386/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 1137.5707\n",
            "Epoch 387/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 16484.1133\n",
            "Epoch 388/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 27286.5664\n",
            "Epoch 389/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 14026.8311\n",
            "Epoch 390/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 60542.9883\n",
            "Epoch 391/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 22.1176\n",
            "Epoch 392/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 11082.6377\n",
            "Epoch 393/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 35386.8516\n",
            "Epoch 394/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 46728.6484\n",
            "Epoch 395/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 18.2155\n",
            "Epoch 396/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 92688.6328\n",
            "Epoch 397/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 9.7098\n",
            "Epoch 398/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 8.9344\n",
            "Epoch 399/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 7493.3066\n",
            "Epoch 400/400\n",
            "293/293 [==============================] - 0s 1ms/step - loss: 60364.7891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9edcc7dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "print(\"The Mean Squared Error on the Test set is: \\t{:0.1f} \".format(mse(y_test,y_pred)))\n",
        "\n",
        "\n",
        "print(\"The R2 score on the Test set is: \\t{:0.3f} \".format(r2_score(y_test,y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kr1swKHgZMW",
        "outputId": "249defb5-f6b6-49a1-f52a-b2756b5f3a78"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mean Squared Error on the Test set is: \t63.9 \n",
            "The R2 score on the Test set is: \t0.957 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANN"
      ],
      "metadata": {
        "id": "oDSdenBsN_Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "iXcNJtbLOJ49"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9Zd8KZTN_e8",
        "outputId": "ae90b49c-faa3-401d-eae4-d4b45a2a6462"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 128)               1664      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 166,529\n",
            "Trainable params: 166,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "yhtGkgw2YRmp"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4YrqEv7QQbk",
        "outputId": "ceccfc94-eae6-4e64-c5de-c633d3a0f585"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 2.4001 - mean_absolute_error: 2.4001\n",
            "Epoch 00001: val_loss improved from inf to 0.59535, saving model to Weights-001--0.59535.hdf5\n",
            "188/188 [==============================] - 2s 5ms/step - loss: 2.3415 - mean_absolute_error: 2.3415 - val_loss: 0.5953 - val_mean_absolute_error: 0.5953\n",
            "Epoch 2/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.6236 - mean_absolute_error: 0.6236\n",
            "Epoch 00002: val_loss improved from 0.59535 to 0.35359, saving model to Weights-002--0.35359.hdf5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6180 - mean_absolute_error: 0.6180 - val_loss: 0.3536 - val_mean_absolute_error: 0.3536\n",
            "Epoch 3/500\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.6864 - mean_absolute_error: 0.6864\n",
            "Epoch 00003: val_loss did not improve from 0.35359\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6613 - mean_absolute_error: 0.6613 - val_loss: 0.4510 - val_mean_absolute_error: 0.4510\n",
            "Epoch 4/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.8787 - mean_absolute_error: 0.8787\n",
            "Epoch 00004: val_loss did not improve from 0.35359\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.8883 - mean_absolute_error: 0.8883 - val_loss: 1.1337 - val_mean_absolute_error: 1.1337\n",
            "Epoch 5/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.6101 - mean_absolute_error: 0.6101\n",
            "Epoch 00005: val_loss did not improve from 0.35359\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6162 - mean_absolute_error: 0.6162 - val_loss: 0.5505 - val_mean_absolute_error: 0.5505\n",
            "Epoch 6/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.3913 - mean_absolute_error: 0.3913\n",
            "Epoch 00006: val_loss did not improve from 0.35359\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3953 - mean_absolute_error: 0.3953 - val_loss: 1.3216 - val_mean_absolute_error: 1.3216\n",
            "Epoch 7/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.6054 - mean_absolute_error: 0.6054\n",
            "Epoch 00007: val_loss did not improve from 0.35359\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6054 - mean_absolute_error: 0.6054 - val_loss: 0.3713 - val_mean_absolute_error: 0.3713\n",
            "Epoch 8/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.5376 - mean_absolute_error: 0.5376\n",
            "Epoch 00008: val_loss did not improve from 0.35359\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.5327 - mean_absolute_error: 0.5327 - val_loss: 0.4922 - val_mean_absolute_error: 0.4922\n",
            "Epoch 9/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.4204 - mean_absolute_error: 0.4204\n",
            "Epoch 00009: val_loss improved from 0.35359 to 0.20686, saving model to Weights-009--0.20686.hdf5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4120 - mean_absolute_error: 0.4120 - val_loss: 0.2069 - val_mean_absolute_error: 0.2069\n",
            "Epoch 10/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.5471 - mean_absolute_error: 0.5471\n",
            "Epoch 00010: val_loss did not improve from 0.20686\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5592 - mean_absolute_error: 0.5592 - val_loss: 1.2263 - val_mean_absolute_error: 1.2263\n",
            "Epoch 11/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.4383 - mean_absolute_error: 0.4383\n",
            "Epoch 00011: val_loss did not improve from 0.20686\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4335 - mean_absolute_error: 0.4335 - val_loss: 0.6832 - val_mean_absolute_error: 0.6832\n",
            "Epoch 12/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.6049 - mean_absolute_error: 0.6049\n",
            "Epoch 00012: val_loss did not improve from 0.20686\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5925 - mean_absolute_error: 0.5925 - val_loss: 0.5999 - val_mean_absolute_error: 0.5999\n",
            "Epoch 13/500\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3916 - mean_absolute_error: 0.3916\n",
            "Epoch 00013: val_loss improved from 0.20686 to 0.11669, saving model to Weights-013--0.11669.hdf5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3820 - mean_absolute_error: 0.3820 - val_loss: 0.1167 - val_mean_absolute_error: 0.1167\n",
            "Epoch 14/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.5013 - mean_absolute_error: 0.5013\n",
            "Epoch 00014: val_loss did not improve from 0.11669\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4883 - mean_absolute_error: 0.4883 - val_loss: 0.2294 - val_mean_absolute_error: 0.2294\n",
            "Epoch 15/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.3466 - mean_absolute_error: 0.3466\n",
            "Epoch 00015: val_loss did not improve from 0.11669\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3466 - mean_absolute_error: 0.3466 - val_loss: 0.2155 - val_mean_absolute_error: 0.2155\n",
            "Epoch 16/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.4257 - mean_absolute_error: 0.4257\n",
            "Epoch 00016: val_loss did not improve from 0.11669\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4158 - mean_absolute_error: 0.4158 - val_loss: 0.9088 - val_mean_absolute_error: 0.9088\n",
            "Epoch 17/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.5182 - mean_absolute_error: 0.5182\n",
            "Epoch 00017: val_loss did not improve from 0.11669\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5030 - mean_absolute_error: 0.5030 - val_loss: 0.6078 - val_mean_absolute_error: 0.6078\n",
            "Epoch 18/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4864 - mean_absolute_error: 0.4864\n",
            "Epoch 00018: val_loss did not improve from 0.11669\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4843 - mean_absolute_error: 0.4843 - val_loss: 0.5911 - val_mean_absolute_error: 0.5911\n",
            "Epoch 19/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.6113 - mean_absolute_error: 0.6113\n",
            "Epoch 00019: val_loss did not improve from 0.11669\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.6082 - mean_absolute_error: 0.6082 - val_loss: 0.7481 - val_mean_absolute_error: 0.7481\n",
            "Epoch 20/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3289 - mean_absolute_error: 0.3289\n",
            "Epoch 00020: val_loss improved from 0.11669 to 0.10691, saving model to Weights-020--0.10691.hdf5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3265 - mean_absolute_error: 0.3265 - val_loss: 0.1069 - val_mean_absolute_error: 0.1069\n",
            "Epoch 21/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3071 - mean_absolute_error: 0.3071\n",
            "Epoch 00021: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3056 - mean_absolute_error: 0.3056 - val_loss: 0.3418 - val_mean_absolute_error: 0.3418\n",
            "Epoch 22/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.3193 - mean_absolute_error: 0.3193\n",
            "Epoch 00022: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3193 - mean_absolute_error: 0.3193 - val_loss: 0.3475 - val_mean_absolute_error: 0.3475\n",
            "Epoch 23/500\n",
            "173/188 [==========================>...] - ETA: 0s - loss: 0.3090 - mean_absolute_error: 0.3090\n",
            "Epoch 00023: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3123 - mean_absolute_error: 0.3123 - val_loss: 0.3544 - val_mean_absolute_error: 0.3544\n",
            "Epoch 24/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3408 - mean_absolute_error: 0.3408\n",
            "Epoch 00024: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3385 - mean_absolute_error: 0.3385 - val_loss: 0.2429 - val_mean_absolute_error: 0.2429\n",
            "Epoch 25/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.4906 - mean_absolute_error: 0.4906\n",
            "Epoch 00025: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4785 - mean_absolute_error: 0.4785 - val_loss: 0.2638 - val_mean_absolute_error: 0.2638\n",
            "Epoch 26/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.4359 - mean_absolute_error: 0.4359\n",
            "Epoch 00026: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4303 - mean_absolute_error: 0.4303 - val_loss: 0.3173 - val_mean_absolute_error: 0.3173\n",
            "Epoch 27/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.5121 - mean_absolute_error: 0.5121\n",
            "Epoch 00027: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4954 - mean_absolute_error: 0.4954 - val_loss: 0.1586 - val_mean_absolute_error: 0.1586\n",
            "Epoch 28/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3626 - mean_absolute_error: 0.3626\n",
            "Epoch 00028: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3596 - mean_absolute_error: 0.3596 - val_loss: 0.3367 - val_mean_absolute_error: 0.3367\n",
            "Epoch 29/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2805 - mean_absolute_error: 0.2805\n",
            "Epoch 00029: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2782 - mean_absolute_error: 0.2782 - val_loss: 0.5875 - val_mean_absolute_error: 0.5875\n",
            "Epoch 30/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3598 - mean_absolute_error: 0.3598\n",
            "Epoch 00030: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3474 - mean_absolute_error: 0.3474 - val_loss: 0.4266 - val_mean_absolute_error: 0.4266\n",
            "Epoch 31/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.3216 - mean_absolute_error: 0.3216\n",
            "Epoch 00031: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3122 - mean_absolute_error: 0.3122 - val_loss: 0.3423 - val_mean_absolute_error: 0.3423\n",
            "Epoch 32/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2723 - mean_absolute_error: 0.2723\n",
            "Epoch 00032: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2718 - mean_absolute_error: 0.2718 - val_loss: 0.2057 - val_mean_absolute_error: 0.2057\n",
            "Epoch 33/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.5767 - mean_absolute_error: 0.5767\n",
            "Epoch 00033: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.5712 - mean_absolute_error: 0.5712 - val_loss: 0.1770 - val_mean_absolute_error: 0.1770\n",
            "Epoch 34/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2686 - mean_absolute_error: 0.2686\n",
            "Epoch 00034: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2674 - mean_absolute_error: 0.2674 - val_loss: 0.1822 - val_mean_absolute_error: 0.1822\n",
            "Epoch 35/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3924 - mean_absolute_error: 0.3924\n",
            "Epoch 00035: val_loss did not improve from 0.10691\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3902 - mean_absolute_error: 0.3902 - val_loss: 0.4305 - val_mean_absolute_error: 0.4305\n",
            "Epoch 36/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.4222 - mean_absolute_error: 0.4222\n",
            "Epoch 00036: val_loss improved from 0.10691 to 0.10613, saving model to Weights-036--0.10613.hdf5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4161 - mean_absolute_error: 0.4161 - val_loss: 0.1061 - val_mean_absolute_error: 0.1061\n",
            "Epoch 37/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.2407 - mean_absolute_error: 0.2407\n",
            "Epoch 00037: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2407 - mean_absolute_error: 0.2407 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
            "Epoch 38/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3801 - mean_absolute_error: 0.3801\n",
            "Epoch 00038: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3762 - mean_absolute_error: 0.3762 - val_loss: 0.2986 - val_mean_absolute_error: 0.2986\n",
            "Epoch 39/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.2801 - mean_absolute_error: 0.2801\n",
            "Epoch 00039: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2801 - mean_absolute_error: 0.2801 - val_loss: 0.1994 - val_mean_absolute_error: 0.1994\n",
            "Epoch 40/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2433 - mean_absolute_error: 0.2433\n",
            "Epoch 00040: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2485 - mean_absolute_error: 0.2485 - val_loss: 0.2231 - val_mean_absolute_error: 0.2231\n",
            "Epoch 41/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2743 - mean_absolute_error: 0.2743\n",
            "Epoch 00041: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2736 - mean_absolute_error: 0.2736 - val_loss: 0.1395 - val_mean_absolute_error: 0.1395\n",
            "Epoch 42/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1943 - mean_absolute_error: 0.1943\n",
            "Epoch 00042: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1937 - mean_absolute_error: 0.1937 - val_loss: 0.2332 - val_mean_absolute_error: 0.2332\n",
            "Epoch 43/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.4745 - mean_absolute_error: 0.4745\n",
            "Epoch 00043: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4632 - mean_absolute_error: 0.4632 - val_loss: 0.1948 - val_mean_absolute_error: 0.1948\n",
            "Epoch 44/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.3064 - mean_absolute_error: 0.3064\n",
            "Epoch 00044: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2987 - mean_absolute_error: 0.2987 - val_loss: 0.2090 - val_mean_absolute_error: 0.2090\n",
            "Epoch 45/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2038 - mean_absolute_error: 0.2038\n",
            "Epoch 00045: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2053 - mean_absolute_error: 0.2053 - val_loss: 0.3705 - val_mean_absolute_error: 0.3705\n",
            "Epoch 46/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.2612 - mean_absolute_error: 0.2612\n",
            "Epoch 00046: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2591 - mean_absolute_error: 0.2591 - val_loss: 0.1850 - val_mean_absolute_error: 0.1850\n",
            "Epoch 47/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.4455 - mean_absolute_error: 0.4455\n",
            "Epoch 00047: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4457 - mean_absolute_error: 0.4457 - val_loss: 0.2146 - val_mean_absolute_error: 0.2146\n",
            "Epoch 48/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2910 - mean_absolute_error: 0.2910\n",
            "Epoch 00048: val_loss did not improve from 0.10613\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2883 - mean_absolute_error: 0.2883 - val_loss: 0.6910 - val_mean_absolute_error: 0.6910\n",
            "Epoch 49/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.3462 - mean_absolute_error: 0.3462\n",
            "Epoch 00049: val_loss improved from 0.10613 to 0.09941, saving model to Weights-049--0.09941.hdf5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3460 - mean_absolute_error: 0.3460 - val_loss: 0.0994 - val_mean_absolute_error: 0.0994\n",
            "Epoch 50/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2904 - mean_absolute_error: 0.2904\n",
            "Epoch 00050: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2854 - mean_absolute_error: 0.2854 - val_loss: 0.1334 - val_mean_absolute_error: 0.1334\n",
            "Epoch 51/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.2020 - mean_absolute_error: 0.2020\n",
            "Epoch 00051: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2008 - mean_absolute_error: 0.2008 - val_loss: 0.2891 - val_mean_absolute_error: 0.2891\n",
            "Epoch 52/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2819 - mean_absolute_error: 0.2819\n",
            "Epoch 00052: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2800 - mean_absolute_error: 0.2800 - val_loss: 0.1488 - val_mean_absolute_error: 0.1488\n",
            "Epoch 53/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2954 - mean_absolute_error: 0.2954\n",
            "Epoch 00053: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2917 - mean_absolute_error: 0.2917 - val_loss: 0.2890 - val_mean_absolute_error: 0.2890\n",
            "Epoch 54/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2252 - mean_absolute_error: 0.2252\n",
            "Epoch 00054: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2266 - mean_absolute_error: 0.2266 - val_loss: 0.2438 - val_mean_absolute_error: 0.2438\n",
            "Epoch 55/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2214 - mean_absolute_error: 0.2214\n",
            "Epoch 00055: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.1065 - val_mean_absolute_error: 0.1065\n",
            "Epoch 56/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1962 - mean_absolute_error: 0.1962\n",
            "Epoch 00056: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1938 - mean_absolute_error: 0.1938 - val_loss: 0.1924 - val_mean_absolute_error: 0.1924\n",
            "Epoch 57/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1968 - mean_absolute_error: 0.1968\n",
            "Epoch 00057: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1982 - mean_absolute_error: 0.1982 - val_loss: 0.2003 - val_mean_absolute_error: 0.2003\n",
            "Epoch 58/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.3019 - mean_absolute_error: 0.3019\n",
            "Epoch 00058: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2925 - mean_absolute_error: 0.2925 - val_loss: 0.2177 - val_mean_absolute_error: 0.2177\n",
            "Epoch 59/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.3033 - mean_absolute_error: 0.3033\n",
            "Epoch 00059: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2969 - mean_absolute_error: 0.2969 - val_loss: 0.1915 - val_mean_absolute_error: 0.1915\n",
            "Epoch 60/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2115 - mean_absolute_error: 0.2115\n",
            "Epoch 00060: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2070 - mean_absolute_error: 0.2070 - val_loss: 0.2808 - val_mean_absolute_error: 0.2808\n",
            "Epoch 61/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2616 - mean_absolute_error: 0.2616\n",
            "Epoch 00061: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2513 - mean_absolute_error: 0.2513 - val_loss: 0.1299 - val_mean_absolute_error: 0.1299\n",
            "Epoch 62/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1872 - mean_absolute_error: 0.1872\n",
            "Epoch 00062: val_loss did not improve from 0.09941\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1856 - mean_absolute_error: 0.1856 - val_loss: 0.1850 - val_mean_absolute_error: 0.1850\n",
            "Epoch 63/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.2081\n",
            "Epoch 00063: val_loss improved from 0.09941 to 0.05700, saving model to Weights-063--0.05700.hdf5\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2125 - mean_absolute_error: 0.2125 - val_loss: 0.0570 - val_mean_absolute_error: 0.0570\n",
            "Epoch 64/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.3191 - mean_absolute_error: 0.3191\n",
            "Epoch 00064: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3190 - mean_absolute_error: 0.3190 - val_loss: 0.1771 - val_mean_absolute_error: 0.1771\n",
            "Epoch 65/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.2249 - mean_absolute_error: 0.2249\n",
            "Epoch 00065: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2242 - mean_absolute_error: 0.2242 - val_loss: 0.1741 - val_mean_absolute_error: 0.1741\n",
            "Epoch 66/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2413 - mean_absolute_error: 0.2413\n",
            "Epoch 00066: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2587 - mean_absolute_error: 0.2587 - val_loss: 0.1828 - val_mean_absolute_error: 0.1828\n",
            "Epoch 67/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.3458 - mean_absolute_error: 0.3458\n",
            "Epoch 00067: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3427 - mean_absolute_error: 0.3427 - val_loss: 0.1023 - val_mean_absolute_error: 0.1023\n",
            "Epoch 68/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.2294 - mean_absolute_error: 0.2294\n",
            "Epoch 00068: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2223 - mean_absolute_error: 0.2223 - val_loss: 0.2692 - val_mean_absolute_error: 0.2692\n",
            "Epoch 69/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.2473 - mean_absolute_error: 0.2473\n",
            "Epoch 00069: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2416 - mean_absolute_error: 0.2416 - val_loss: 0.0788 - val_mean_absolute_error: 0.0788\n",
            "Epoch 70/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.4377 - mean_absolute_error: 0.4377\n",
            "Epoch 00070: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4252 - mean_absolute_error: 0.4252 - val_loss: 0.3965 - val_mean_absolute_error: 0.3965\n",
            "Epoch 71/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2938 - mean_absolute_error: 0.2938\n",
            "Epoch 00071: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3059 - mean_absolute_error: 0.3059 - val_loss: 0.0830 - val_mean_absolute_error: 0.0830\n",
            "Epoch 72/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2511 - mean_absolute_error: 0.2511\n",
            "Epoch 00072: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2435 - mean_absolute_error: 0.2435 - val_loss: 0.1299 - val_mean_absolute_error: 0.1299\n",
            "Epoch 73/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2096 - mean_absolute_error: 0.2096\n",
            "Epoch 00073: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2046 - mean_absolute_error: 0.2046 - val_loss: 0.4338 - val_mean_absolute_error: 0.4338\n",
            "Epoch 74/500\n",
            "172/188 [==========================>...] - ETA: 0s - loss: 0.2274 - mean_absolute_error: 0.2274\n",
            "Epoch 00074: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2210 - mean_absolute_error: 0.2210 - val_loss: 0.2184 - val_mean_absolute_error: 0.2184\n",
            "Epoch 75/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.4398 - mean_absolute_error: 0.4398\n",
            "Epoch 00075: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4398 - mean_absolute_error: 0.4398 - val_loss: 0.4241 - val_mean_absolute_error: 0.4241\n",
            "Epoch 76/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.4091 - mean_absolute_error: 0.4091\n",
            "Epoch 00076: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3979 - mean_absolute_error: 0.3979 - val_loss: 0.1040 - val_mean_absolute_error: 0.1040\n",
            "Epoch 77/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.2117 - mean_absolute_error: 0.2117\n",
            "Epoch 00077: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2131 - mean_absolute_error: 0.2131 - val_loss: 0.1054 - val_mean_absolute_error: 0.1054\n",
            "Epoch 78/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.1985 - mean_absolute_error: 0.1985\n",
            "Epoch 00078: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1969 - mean_absolute_error: 0.1969 - val_loss: 0.2278 - val_mean_absolute_error: 0.2278\n",
            "Epoch 79/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2050 - mean_absolute_error: 0.2050\n",
            "Epoch 00079: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2012 - mean_absolute_error: 0.2012 - val_loss: 0.1377 - val_mean_absolute_error: 0.1377\n",
            "Epoch 80/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1839 - mean_absolute_error: 0.1839\n",
            "Epoch 00080: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1858 - mean_absolute_error: 0.1858 - val_loss: 0.3081 - val_mean_absolute_error: 0.3081\n",
            "Epoch 81/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1892 - mean_absolute_error: 0.1892\n",
            "Epoch 00081: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1901 - mean_absolute_error: 0.1901 - val_loss: 0.6055 - val_mean_absolute_error: 0.6055\n",
            "Epoch 82/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2682 - mean_absolute_error: 0.2682\n",
            "Epoch 00082: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2672 - mean_absolute_error: 0.2672 - val_loss: 0.1351 - val_mean_absolute_error: 0.1351\n",
            "Epoch 83/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2104 - mean_absolute_error: 0.2104\n",
            "Epoch 00083: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2088 - mean_absolute_error: 0.2088 - val_loss: 0.0863 - val_mean_absolute_error: 0.0863\n",
            "Epoch 84/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1781 - mean_absolute_error: 0.1781\n",
            "Epoch 00084: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1768 - mean_absolute_error: 0.1768 - val_loss: 0.0899 - val_mean_absolute_error: 0.0899\n",
            "Epoch 85/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.2135 - mean_absolute_error: 0.2135\n",
            "Epoch 00085: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2135 - mean_absolute_error: 0.2135 - val_loss: 0.3158 - val_mean_absolute_error: 0.3158\n",
            "Epoch 86/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1941 - mean_absolute_error: 0.1941\n",
            "Epoch 00086: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1906 - mean_absolute_error: 0.1906 - val_loss: 0.1107 - val_mean_absolute_error: 0.1107\n",
            "Epoch 87/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1865 - mean_absolute_error: 0.1865\n",
            "Epoch 00087: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1864 - mean_absolute_error: 0.1864 - val_loss: 0.1317 - val_mean_absolute_error: 0.1317\n",
            "Epoch 88/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2724 - mean_absolute_error: 0.2724\n",
            "Epoch 00088: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2690 - mean_absolute_error: 0.2690 - val_loss: 0.1529 - val_mean_absolute_error: 0.1529\n",
            "Epoch 89/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1534 - mean_absolute_error: 0.1534\n",
            "Epoch 00089: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1507 - mean_absolute_error: 0.1507 - val_loss: 0.1021 - val_mean_absolute_error: 0.1021\n",
            "Epoch 90/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1808 - mean_absolute_error: 0.1808\n",
            "Epoch 00090: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1794 - mean_absolute_error: 0.1794 - val_loss: 0.3307 - val_mean_absolute_error: 0.3307\n",
            "Epoch 91/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2498 - mean_absolute_error: 0.2498\n",
            "Epoch 00091: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2503 - mean_absolute_error: 0.2503 - val_loss: 0.1970 - val_mean_absolute_error: 0.1970\n",
            "Epoch 92/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1799 - mean_absolute_error: 0.1799\n",
            "Epoch 00092: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1791 - mean_absolute_error: 0.1791 - val_loss: 0.2755 - val_mean_absolute_error: 0.2755\n",
            "Epoch 93/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1971 - mean_absolute_error: 0.1971\n",
            "Epoch 00093: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1943 - mean_absolute_error: 0.1943 - val_loss: 0.1969 - val_mean_absolute_error: 0.1969\n",
            "Epoch 94/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1672 - mean_absolute_error: 0.1672\n",
            "Epoch 00094: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1660 - mean_absolute_error: 0.1660 - val_loss: 0.2558 - val_mean_absolute_error: 0.2558\n",
            "Epoch 95/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1966 - mean_absolute_error: 0.1966\n",
            "Epoch 00095: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1965 - mean_absolute_error: 0.1965 - val_loss: 0.3636 - val_mean_absolute_error: 0.3636\n",
            "Epoch 96/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2456 - mean_absolute_error: 0.2456\n",
            "Epoch 00096: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2438 - mean_absolute_error: 0.2438 - val_loss: 0.1786 - val_mean_absolute_error: 0.1786\n",
            "Epoch 97/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1538 - mean_absolute_error: 0.1538\n",
            "Epoch 00097: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1540 - mean_absolute_error: 0.1540 - val_loss: 0.1534 - val_mean_absolute_error: 0.1534\n",
            "Epoch 98/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2202 - mean_absolute_error: 0.2202\n",
            "Epoch 00098: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2215 - mean_absolute_error: 0.2215 - val_loss: 0.2956 - val_mean_absolute_error: 0.2956\n",
            "Epoch 99/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2442 - mean_absolute_error: 0.2442\n",
            "Epoch 00099: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2422 - mean_absolute_error: 0.2422 - val_loss: 0.6417 - val_mean_absolute_error: 0.6417\n",
            "Epoch 100/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2513 - mean_absolute_error: 0.2513\n",
            "Epoch 00100: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2504 - mean_absolute_error: 0.2504 - val_loss: 0.1306 - val_mean_absolute_error: 0.1306\n",
            "Epoch 101/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1900 - mean_absolute_error: 0.1900\n",
            "Epoch 00101: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1828 - mean_absolute_error: 0.1828 - val_loss: 0.0990 - val_mean_absolute_error: 0.0990\n",
            "Epoch 102/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2340 - mean_absolute_error: 0.2340\n",
            "Epoch 00102: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2698 - val_mean_absolute_error: 0.2698\n",
            "Epoch 103/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2038 - mean_absolute_error: 0.2038\n",
            "Epoch 00103: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2036 - mean_absolute_error: 0.2036 - val_loss: 0.1173 - val_mean_absolute_error: 0.1173\n",
            "Epoch 104/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1429 - mean_absolute_error: 0.1429\n",
            "Epoch 00104: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1493 - mean_absolute_error: 0.1493 - val_loss: 0.2119 - val_mean_absolute_error: 0.2119\n",
            "Epoch 105/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.2085 - mean_absolute_error: 0.2085\n",
            "Epoch 00105: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2084 - mean_absolute_error: 0.2084 - val_loss: 0.3734 - val_mean_absolute_error: 0.3734\n",
            "Epoch 106/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2234 - mean_absolute_error: 0.2234\n",
            "Epoch 00106: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.0576 - val_mean_absolute_error: 0.0576\n",
            "Epoch 107/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.2695 - mean_absolute_error: 0.2695\n",
            "Epoch 00107: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2677 - mean_absolute_error: 0.2677 - val_loss: 0.4588 - val_mean_absolute_error: 0.4588\n",
            "Epoch 108/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1795 - mean_absolute_error: 0.1795\n",
            "Epoch 00108: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1740 - mean_absolute_error: 0.1740 - val_loss: 0.1416 - val_mean_absolute_error: 0.1416\n",
            "Epoch 109/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2456 - mean_absolute_error: 0.2456\n",
            "Epoch 00109: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2440 - mean_absolute_error: 0.2440 - val_loss: 0.2066 - val_mean_absolute_error: 0.2066\n",
            "Epoch 110/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.2631 - mean_absolute_error: 0.2631\n",
            "Epoch 00110: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2642 - mean_absolute_error: 0.2642 - val_loss: 0.2246 - val_mean_absolute_error: 0.2246\n",
            "Epoch 111/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.2147 - mean_absolute_error: 0.2147\n",
            "Epoch 00111: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2102 - mean_absolute_error: 0.2102 - val_loss: 0.0725 - val_mean_absolute_error: 0.0725\n",
            "Epoch 112/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.2932 - mean_absolute_error: 0.2932\n",
            "Epoch 00112: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2932 - mean_absolute_error: 0.2932 - val_loss: 0.5577 - val_mean_absolute_error: 0.5577\n",
            "Epoch 113/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2518 - mean_absolute_error: 0.2518\n",
            "Epoch 00113: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2471 - mean_absolute_error: 0.2471 - val_loss: 0.1359 - val_mean_absolute_error: 0.1359\n",
            "Epoch 114/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2626 - mean_absolute_error: 0.2626\n",
            "Epoch 00114: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2652 - mean_absolute_error: 0.2652 - val_loss: 0.3481 - val_mean_absolute_error: 0.3481\n",
            "Epoch 115/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2681 - mean_absolute_error: 0.2681\n",
            "Epoch 00115: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2646 - mean_absolute_error: 0.2646 - val_loss: 0.2297 - val_mean_absolute_error: 0.2297\n",
            "Epoch 116/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1728 - mean_absolute_error: 0.1728\n",
            "Epoch 00116: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1718 - mean_absolute_error: 0.1718 - val_loss: 0.0912 - val_mean_absolute_error: 0.0912\n",
            "Epoch 117/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1769 - mean_absolute_error: 0.1769\n",
            "Epoch 00117: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1717 - mean_absolute_error: 0.1717 - val_loss: 0.1619 - val_mean_absolute_error: 0.1619\n",
            "Epoch 118/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1615 - mean_absolute_error: 0.1615\n",
            "Epoch 00118: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1595 - mean_absolute_error: 0.1595 - val_loss: 0.1571 - val_mean_absolute_error: 0.1571\n",
            "Epoch 119/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2417 - mean_absolute_error: 0.2417\n",
            "Epoch 00119: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2376 - mean_absolute_error: 0.2376 - val_loss: 0.1398 - val_mean_absolute_error: 0.1398\n",
            "Epoch 120/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1461 - mean_absolute_error: 0.1461\n",
            "Epoch 00120: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1510 - mean_absolute_error: 0.1510 - val_loss: 0.7094 - val_mean_absolute_error: 0.7094\n",
            "Epoch 121/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2681 - mean_absolute_error: 0.2681\n",
            "Epoch 00121: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2644 - mean_absolute_error: 0.2644 - val_loss: 0.2163 - val_mean_absolute_error: 0.2163\n",
            "Epoch 122/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.2245 - mean_absolute_error: 0.2245\n",
            "Epoch 00122: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2243 - mean_absolute_error: 0.2243 - val_loss: 0.3043 - val_mean_absolute_error: 0.3043\n",
            "Epoch 123/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.3047 - mean_absolute_error: 0.3047\n",
            "Epoch 00123: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3041 - mean_absolute_error: 0.3041 - val_loss: 0.3915 - val_mean_absolute_error: 0.3915\n",
            "Epoch 124/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.4283 - mean_absolute_error: 0.4283\n",
            "Epoch 00124: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.4232 - mean_absolute_error: 0.4232 - val_loss: 0.2111 - val_mean_absolute_error: 0.2111\n",
            "Epoch 125/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1666 - mean_absolute_error: 0.1666\n",
            "Epoch 00125: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1665 - mean_absolute_error: 0.1665 - val_loss: 0.1007 - val_mean_absolute_error: 0.1007\n",
            "Epoch 126/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1655 - mean_absolute_error: 0.1655\n",
            "Epoch 00126: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1635 - mean_absolute_error: 0.1635 - val_loss: 0.0644 - val_mean_absolute_error: 0.0644\n",
            "Epoch 127/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1523 - mean_absolute_error: 0.1523\n",
            "Epoch 00127: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1520 - mean_absolute_error: 0.1520 - val_loss: 0.2770 - val_mean_absolute_error: 0.2770\n",
            "Epoch 128/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2409 - mean_absolute_error: 0.2409\n",
            "Epoch 00128: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2396 - mean_absolute_error: 0.2396 - val_loss: 0.1657 - val_mean_absolute_error: 0.1657\n",
            "Epoch 129/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1851 - mean_absolute_error: 0.1851\n",
            "Epoch 00129: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1806 - mean_absolute_error: 0.1806 - val_loss: 0.1861 - val_mean_absolute_error: 0.1861\n",
            "Epoch 130/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1710 - mean_absolute_error: 0.1710\n",
            "Epoch 00130: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1710 - mean_absolute_error: 0.1710 - val_loss: 0.1530 - val_mean_absolute_error: 0.1530\n",
            "Epoch 131/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2878 - mean_absolute_error: 0.2878\n",
            "Epoch 00131: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2851 - mean_absolute_error: 0.2851 - val_loss: 0.2139 - val_mean_absolute_error: 0.2139\n",
            "Epoch 132/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1634 - mean_absolute_error: 0.1634\n",
            "Epoch 00132: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1612 - mean_absolute_error: 0.1612 - val_loss: 0.1471 - val_mean_absolute_error: 0.1471\n",
            "Epoch 133/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1809 - mean_absolute_error: 0.1809\n",
            "Epoch 00133: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1791 - mean_absolute_error: 0.1791 - val_loss: 0.2719 - val_mean_absolute_error: 0.2719\n",
            "Epoch 134/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2442 - mean_absolute_error: 0.2442\n",
            "Epoch 00134: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2428 - mean_absolute_error: 0.2428 - val_loss: 0.2854 - val_mean_absolute_error: 0.2854\n",
            "Epoch 135/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1426 - mean_absolute_error: 0.1426\n",
            "Epoch 00135: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1437 - mean_absolute_error: 0.1437 - val_loss: 0.1128 - val_mean_absolute_error: 0.1128\n",
            "Epoch 136/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1320 - mean_absolute_error: 0.1320\n",
            "Epoch 00136: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1319 - mean_absolute_error: 0.1319 - val_loss: 0.1476 - val_mean_absolute_error: 0.1476\n",
            "Epoch 137/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1734 - mean_absolute_error: 0.1734\n",
            "Epoch 00137: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1690 - mean_absolute_error: 0.1690 - val_loss: 0.0997 - val_mean_absolute_error: 0.0997\n",
            "Epoch 138/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1515 - mean_absolute_error: 0.1515\n",
            "Epoch 00138: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1514 - mean_absolute_error: 0.1514 - val_loss: 0.1514 - val_mean_absolute_error: 0.1514\n",
            "Epoch 139/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1862 - mean_absolute_error: 0.1862\n",
            "Epoch 00139: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1854 - mean_absolute_error: 0.1854 - val_loss: 0.1816 - val_mean_absolute_error: 0.1816\n",
            "Epoch 140/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1804 - mean_absolute_error: 0.1804\n",
            "Epoch 00140: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1870 - mean_absolute_error: 0.1870 - val_loss: 0.3690 - val_mean_absolute_error: 0.3690\n",
            "Epoch 141/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1930 - mean_absolute_error: 0.1930\n",
            "Epoch 00141: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1924 - mean_absolute_error: 0.1924 - val_loss: 0.2728 - val_mean_absolute_error: 0.2728\n",
            "Epoch 142/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1644 - mean_absolute_error: 0.1644\n",
            "Epoch 00142: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1664 - mean_absolute_error: 0.1664 - val_loss: 0.3144 - val_mean_absolute_error: 0.3144\n",
            "Epoch 143/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1375 - mean_absolute_error: 0.1375\n",
            "Epoch 00143: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1351 - mean_absolute_error: 0.1351 - val_loss: 0.0782 - val_mean_absolute_error: 0.0782\n",
            "Epoch 144/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.2578 - mean_absolute_error: 0.2578\n",
            "Epoch 00144: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2581 - mean_absolute_error: 0.2581 - val_loss: 0.1720 - val_mean_absolute_error: 0.1720\n",
            "Epoch 145/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1592 - mean_absolute_error: 0.1592\n",
            "Epoch 00145: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1741 - mean_absolute_error: 0.1741 - val_loss: 0.0961 - val_mean_absolute_error: 0.0961\n",
            "Epoch 146/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2125 - mean_absolute_error: 0.2125\n",
            "Epoch 00146: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.3225 - val_mean_absolute_error: 0.3225\n",
            "Epoch 147/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.3000 - mean_absolute_error: 0.3000\n",
            "Epoch 00147: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2970 - mean_absolute_error: 0.2970 - val_loss: 0.2976 - val_mean_absolute_error: 0.2976\n",
            "Epoch 148/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1617 - mean_absolute_error: 0.1617\n",
            "Epoch 00148: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1612 - mean_absolute_error: 0.1612 - val_loss: 0.3004 - val_mean_absolute_error: 0.3004\n",
            "Epoch 149/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1625 - mean_absolute_error: 0.1625\n",
            "Epoch 00149: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1647 - mean_absolute_error: 0.1647 - val_loss: 0.0913 - val_mean_absolute_error: 0.0913\n",
            "Epoch 150/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1669 - mean_absolute_error: 0.1669\n",
            "Epoch 00150: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1669 - mean_absolute_error: 0.1669 - val_loss: 0.3118 - val_mean_absolute_error: 0.3118\n",
            "Epoch 151/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1832 - mean_absolute_error: 0.1832\n",
            "Epoch 00151: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1845 - mean_absolute_error: 0.1845 - val_loss: 0.4112 - val_mean_absolute_error: 0.4112\n",
            "Epoch 152/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.3823 - mean_absolute_error: 0.3823\n",
            "Epoch 00152: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3692 - mean_absolute_error: 0.3692 - val_loss: 0.2130 - val_mean_absolute_error: 0.2130\n",
            "Epoch 153/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1404 - mean_absolute_error: 0.1404\n",
            "Epoch 00153: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1403 - mean_absolute_error: 0.1403 - val_loss: 0.1752 - val_mean_absolute_error: 0.1752\n",
            "Epoch 154/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1690 - mean_absolute_error: 0.1690\n",
            "Epoch 00154: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1689 - mean_absolute_error: 0.1689 - val_loss: 0.1407 - val_mean_absolute_error: 0.1407\n",
            "Epoch 155/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1610 - mean_absolute_error: 0.1610\n",
            "Epoch 00155: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1610 - mean_absolute_error: 0.1610 - val_loss: 0.0682 - val_mean_absolute_error: 0.0682\n",
            "Epoch 156/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.1525 - mean_absolute_error: 0.1525\n",
            "Epoch 00156: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1600 - mean_absolute_error: 0.1600 - val_loss: 0.1159 - val_mean_absolute_error: 0.1159\n",
            "Epoch 157/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2009 - mean_absolute_error: 0.2009\n",
            "Epoch 00157: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2021 - mean_absolute_error: 0.2021 - val_loss: 0.1532 - val_mean_absolute_error: 0.1532\n",
            "Epoch 158/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1337 - mean_absolute_error: 0.1337\n",
            "Epoch 00158: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1347 - mean_absolute_error: 0.1347 - val_loss: 0.1232 - val_mean_absolute_error: 0.1232\n",
            "Epoch 159/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1589 - mean_absolute_error: 0.1589\n",
            "Epoch 00159: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1604 - mean_absolute_error: 0.1604 - val_loss: 0.1566 - val_mean_absolute_error: 0.1566\n",
            "Epoch 160/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1759 - mean_absolute_error: 0.1759\n",
            "Epoch 00160: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1770 - mean_absolute_error: 0.1770 - val_loss: 0.1719 - val_mean_absolute_error: 0.1719\n",
            "Epoch 161/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.2648 - mean_absolute_error: 0.2648\n",
            "Epoch 00161: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2648 - mean_absolute_error: 0.2648 - val_loss: 0.0939 - val_mean_absolute_error: 0.0939\n",
            "Epoch 162/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1390 - mean_absolute_error: 0.1390\n",
            "Epoch 00162: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1402 - mean_absolute_error: 0.1402 - val_loss: 0.0909 - val_mean_absolute_error: 0.0909\n",
            "Epoch 163/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1887 - mean_absolute_error: 0.1887\n",
            "Epoch 00163: val_loss did not improve from 0.05700\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1879 - mean_absolute_error: 0.1879 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
            "Epoch 164/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1681 - mean_absolute_error: 0.1681\n",
            "Epoch 00164: val_loss improved from 0.05700 to 0.04656, saving model to Weights-164--0.04656.hdf5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1658 - mean_absolute_error: 0.1658 - val_loss: 0.0466 - val_mean_absolute_error: 0.0466\n",
            "Epoch 165/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1116 - mean_absolute_error: 0.1116\n",
            "Epoch 00165: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1116 - mean_absolute_error: 0.1116 - val_loss: 0.1530 - val_mean_absolute_error: 0.1530\n",
            "Epoch 166/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2212 - mean_absolute_error: 0.2212\n",
            "Epoch 00166: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2244 - mean_absolute_error: 0.2244 - val_loss: 0.2674 - val_mean_absolute_error: 0.2674\n",
            "Epoch 167/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1832 - mean_absolute_error: 0.1832\n",
            "Epoch 00167: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1830 - mean_absolute_error: 0.1830 - val_loss: 0.2337 - val_mean_absolute_error: 0.2337\n",
            "Epoch 168/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.2499 - mean_absolute_error: 0.2499\n",
            "Epoch 00168: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2393 - mean_absolute_error: 0.2393 - val_loss: 0.2635 - val_mean_absolute_error: 0.2635\n",
            "Epoch 169/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1366 - mean_absolute_error: 0.1366\n",
            "Epoch 00169: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1354 - mean_absolute_error: 0.1354 - val_loss: 0.1314 - val_mean_absolute_error: 0.1314\n",
            "Epoch 170/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.2224 - mean_absolute_error: 0.2224\n",
            "Epoch 00170: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2219 - mean_absolute_error: 0.2219 - val_loss: 0.0669 - val_mean_absolute_error: 0.0669\n",
            "Epoch 171/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1382 - mean_absolute_error: 0.1382\n",
            "Epoch 00171: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1374 - mean_absolute_error: 0.1374 - val_loss: 0.2795 - val_mean_absolute_error: 0.2795\n",
            "Epoch 172/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1850 - mean_absolute_error: 0.1850\n",
            "Epoch 00172: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1822 - mean_absolute_error: 0.1822 - val_loss: 0.1462 - val_mean_absolute_error: 0.1462\n",
            "Epoch 173/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1401 - mean_absolute_error: 0.1401\n",
            "Epoch 00173: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1400 - mean_absolute_error: 0.1400 - val_loss: 0.2395 - val_mean_absolute_error: 0.2395\n",
            "Epoch 174/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1816 - mean_absolute_error: 0.1816\n",
            "Epoch 00174: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1782 - mean_absolute_error: 0.1782 - val_loss: 0.1583 - val_mean_absolute_error: 0.1583\n",
            "Epoch 175/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1677 - mean_absolute_error: 0.1677\n",
            "Epoch 00175: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1648 - mean_absolute_error: 0.1648 - val_loss: 0.1709 - val_mean_absolute_error: 0.1709\n",
            "Epoch 176/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.2530 - mean_absolute_error: 0.2530\n",
            "Epoch 00176: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2651 - mean_absolute_error: 0.2651 - val_loss: 0.2202 - val_mean_absolute_error: 0.2202\n",
            "Epoch 177/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2434 - mean_absolute_error: 0.2434\n",
            "Epoch 00177: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2424 - mean_absolute_error: 0.2424 - val_loss: 0.0777 - val_mean_absolute_error: 0.0777\n",
            "Epoch 178/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1478 - mean_absolute_error: 0.1478\n",
            "Epoch 00178: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1472 - mean_absolute_error: 0.1472 - val_loss: 0.3941 - val_mean_absolute_error: 0.3941\n",
            "Epoch 179/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1833 - mean_absolute_error: 0.1833\n",
            "Epoch 00179: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1816 - mean_absolute_error: 0.1816 - val_loss: 0.2132 - val_mean_absolute_error: 0.2132\n",
            "Epoch 180/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1811 - mean_absolute_error: 0.1811\n",
            "Epoch 00180: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1820 - mean_absolute_error: 0.1820 - val_loss: 0.1870 - val_mean_absolute_error: 0.1870\n",
            "Epoch 181/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1544 - mean_absolute_error: 0.1544\n",
            "Epoch 00181: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1535 - mean_absolute_error: 0.1535 - val_loss: 0.1695 - val_mean_absolute_error: 0.1695\n",
            "Epoch 182/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.2114 - mean_absolute_error: 0.2114\n",
            "Epoch 00182: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2116 - mean_absolute_error: 0.2116 - val_loss: 0.1089 - val_mean_absolute_error: 0.1089\n",
            "Epoch 183/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1868 - mean_absolute_error: 0.1868\n",
            "Epoch 00183: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1876 - mean_absolute_error: 0.1876 - val_loss: 0.0776 - val_mean_absolute_error: 0.0776\n",
            "Epoch 184/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1689 - mean_absolute_error: 0.1689\n",
            "Epoch 00184: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1646 - mean_absolute_error: 0.1646 - val_loss: 0.1262 - val_mean_absolute_error: 0.1262\n",
            "Epoch 185/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1428 - mean_absolute_error: 0.1428\n",
            "Epoch 00185: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1391 - mean_absolute_error: 0.1391 - val_loss: 0.0486 - val_mean_absolute_error: 0.0486\n",
            "Epoch 186/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1320 - mean_absolute_error: 0.1320\n",
            "Epoch 00186: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1315 - mean_absolute_error: 0.1315 - val_loss: 0.0977 - val_mean_absolute_error: 0.0977\n",
            "Epoch 187/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1993 - mean_absolute_error: 0.1993\n",
            "Epoch 00187: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1993 - mean_absolute_error: 0.1993 - val_loss: 0.0750 - val_mean_absolute_error: 0.0750\n",
            "Epoch 188/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1189 - mean_absolute_error: 0.1189\n",
            "Epoch 00188: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1191 - mean_absolute_error: 0.1191 - val_loss: 0.4035 - val_mean_absolute_error: 0.4035\n",
            "Epoch 189/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1582 - mean_absolute_error: 0.1582\n",
            "Epoch 00189: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1570 - mean_absolute_error: 0.1570 - val_loss: 0.1235 - val_mean_absolute_error: 0.1235\n",
            "Epoch 190/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1584 - mean_absolute_error: 0.1584\n",
            "Epoch 00190: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1596 - mean_absolute_error: 0.1596 - val_loss: 0.1639 - val_mean_absolute_error: 0.1639\n",
            "Epoch 191/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1500 - mean_absolute_error: 0.1500\n",
            "Epoch 00191: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1514 - mean_absolute_error: 0.1514 - val_loss: 0.0813 - val_mean_absolute_error: 0.0813\n",
            "Epoch 192/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1688 - mean_absolute_error: 0.1688\n",
            "Epoch 00192: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1688 - mean_absolute_error: 0.1688 - val_loss: 0.0698 - val_mean_absolute_error: 0.0698\n",
            "Epoch 193/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1290 - mean_absolute_error: 0.1290\n",
            "Epoch 00193: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1295 - mean_absolute_error: 0.1295 - val_loss: 0.0956 - val_mean_absolute_error: 0.0956\n",
            "Epoch 194/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1289 - mean_absolute_error: 0.1289\n",
            "Epoch 00194: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1283 - mean_absolute_error: 0.1283 - val_loss: 0.0957 - val_mean_absolute_error: 0.0957\n",
            "Epoch 195/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1367 - mean_absolute_error: 0.1367\n",
            "Epoch 00195: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1366 - mean_absolute_error: 0.1366 - val_loss: 0.3045 - val_mean_absolute_error: 0.3045\n",
            "Epoch 196/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1554 - mean_absolute_error: 0.1554\n",
            "Epoch 00196: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1543 - mean_absolute_error: 0.1543 - val_loss: 0.1581 - val_mean_absolute_error: 0.1581\n",
            "Epoch 197/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1188 - mean_absolute_error: 0.1188\n",
            "Epoch 00197: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1166 - mean_absolute_error: 0.1166 - val_loss: 0.1852 - val_mean_absolute_error: 0.1852\n",
            "Epoch 198/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1489 - mean_absolute_error: 0.1489\n",
            "Epoch 00198: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1486 - mean_absolute_error: 0.1486 - val_loss: 0.2274 - val_mean_absolute_error: 0.2274\n",
            "Epoch 199/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2076 - mean_absolute_error: 0.2076\n",
            "Epoch 00199: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2066 - mean_absolute_error: 0.2066 - val_loss: 0.0541 - val_mean_absolute_error: 0.0541\n",
            "Epoch 200/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1428 - mean_absolute_error: 0.1428\n",
            "Epoch 00200: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1403 - mean_absolute_error: 0.1403 - val_loss: 0.1812 - val_mean_absolute_error: 0.1812\n",
            "Epoch 201/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1648 - mean_absolute_error: 0.1648\n",
            "Epoch 00201: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1609 - mean_absolute_error: 0.1609 - val_loss: 0.0581 - val_mean_absolute_error: 0.0581\n",
            "Epoch 202/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1528 - mean_absolute_error: 0.1528\n",
            "Epoch 00202: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1509 - mean_absolute_error: 0.1509 - val_loss: 0.0721 - val_mean_absolute_error: 0.0721\n",
            "Epoch 203/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1613 - mean_absolute_error: 0.1613\n",
            "Epoch 00203: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1564 - mean_absolute_error: 0.1564 - val_loss: 0.0628 - val_mean_absolute_error: 0.0628\n",
            "Epoch 204/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1722 - mean_absolute_error: 0.1722\n",
            "Epoch 00204: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1725 - mean_absolute_error: 0.1725 - val_loss: 0.2077 - val_mean_absolute_error: 0.2077\n",
            "Epoch 205/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1274 - mean_absolute_error: 0.1274\n",
            "Epoch 00205: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1264 - mean_absolute_error: 0.1264 - val_loss: 0.0637 - val_mean_absolute_error: 0.0637\n",
            "Epoch 206/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1556 - mean_absolute_error: 0.1556\n",
            "Epoch 00206: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1531 - mean_absolute_error: 0.1531 - val_loss: 0.1867 - val_mean_absolute_error: 0.1867\n",
            "Epoch 207/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1224 - mean_absolute_error: 0.1224\n",
            "Epoch 00207: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1199 - mean_absolute_error: 0.1199 - val_loss: 0.0693 - val_mean_absolute_error: 0.0693\n",
            "Epoch 208/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1916 - mean_absolute_error: 0.1916\n",
            "Epoch 00208: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1948 - mean_absolute_error: 0.1948 - val_loss: 0.0938 - val_mean_absolute_error: 0.0938\n",
            "Epoch 209/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1350 - mean_absolute_error: 0.1350\n",
            "Epoch 00209: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1321 - mean_absolute_error: 0.1321 - val_loss: 0.1238 - val_mean_absolute_error: 0.1238\n",
            "Epoch 210/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1540 - mean_absolute_error: 0.1540\n",
            "Epoch 00210: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1542 - mean_absolute_error: 0.1542 - val_loss: 0.2080 - val_mean_absolute_error: 0.2080\n",
            "Epoch 211/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1833 - mean_absolute_error: 0.1833\n",
            "Epoch 00211: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1833 - mean_absolute_error: 0.1833 - val_loss: 0.1806 - val_mean_absolute_error: 0.1806\n",
            "Epoch 212/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1559 - mean_absolute_error: 0.1559\n",
            "Epoch 00212: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1551 - mean_absolute_error: 0.1551 - val_loss: 0.1736 - val_mean_absolute_error: 0.1736\n",
            "Epoch 213/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.2441 - mean_absolute_error: 0.2441\n",
            "Epoch 00213: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2475 - mean_absolute_error: 0.2475 - val_loss: 0.3866 - val_mean_absolute_error: 0.3866\n",
            "Epoch 214/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1353 - mean_absolute_error: 0.1353\n",
            "Epoch 00214: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1416 - mean_absolute_error: 0.1416 - val_loss: 0.2938 - val_mean_absolute_error: 0.2938\n",
            "Epoch 215/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1493 - mean_absolute_error: 0.1493\n",
            "Epoch 00215: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1498 - mean_absolute_error: 0.1498 - val_loss: 0.5163 - val_mean_absolute_error: 0.5163\n",
            "Epoch 216/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1487 - mean_absolute_error: 0.1487\n",
            "Epoch 00216: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1531 - mean_absolute_error: 0.1531 - val_loss: 0.2629 - val_mean_absolute_error: 0.2629\n",
            "Epoch 217/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1289 - mean_absolute_error: 0.1289\n",
            "Epoch 00217: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1254 - mean_absolute_error: 0.1254 - val_loss: 0.0632 - val_mean_absolute_error: 0.0632\n",
            "Epoch 218/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1477 - mean_absolute_error: 0.1477\n",
            "Epoch 00218: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1464 - mean_absolute_error: 0.1464 - val_loss: 0.0982 - val_mean_absolute_error: 0.0982\n",
            "Epoch 219/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1122 - mean_absolute_error: 0.1122\n",
            "Epoch 00219: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1119 - mean_absolute_error: 0.1119 - val_loss: 0.0705 - val_mean_absolute_error: 0.0705\n",
            "Epoch 220/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1504 - mean_absolute_error: 0.1504\n",
            "Epoch 00220: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.1749 - val_mean_absolute_error: 0.1749\n",
            "Epoch 221/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1342 - mean_absolute_error: 0.1342\n",
            "Epoch 00221: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1328 - mean_absolute_error: 0.1328 - val_loss: 0.1331 - val_mean_absolute_error: 0.1331\n",
            "Epoch 222/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1178 - mean_absolute_error: 0.1178\n",
            "Epoch 00222: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1182 - mean_absolute_error: 0.1182 - val_loss: 0.0993 - val_mean_absolute_error: 0.0993\n",
            "Epoch 223/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1995 - mean_absolute_error: 0.1995\n",
            "Epoch 00223: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1962 - mean_absolute_error: 0.1962 - val_loss: 0.1145 - val_mean_absolute_error: 0.1145\n",
            "Epoch 224/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1503 - mean_absolute_error: 0.1503\n",
            "Epoch 00224: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1502 - mean_absolute_error: 0.1502 - val_loss: 0.4195 - val_mean_absolute_error: 0.4195\n",
            "Epoch 225/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1684 - mean_absolute_error: 0.1684\n",
            "Epoch 00225: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1676 - mean_absolute_error: 0.1676 - val_loss: 0.0742 - val_mean_absolute_error: 0.0742\n",
            "Epoch 226/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1137 - mean_absolute_error: 0.1137\n",
            "Epoch 00226: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1136 - mean_absolute_error: 0.1136 - val_loss: 0.1599 - val_mean_absolute_error: 0.1599\n",
            "Epoch 227/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1259 - mean_absolute_error: 0.1259\n",
            "Epoch 00227: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1229 - mean_absolute_error: 0.1229 - val_loss: 0.0708 - val_mean_absolute_error: 0.0708\n",
            "Epoch 228/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.0980 - mean_absolute_error: 0.0980\n",
            "Epoch 00228: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0967 - mean_absolute_error: 0.0967 - val_loss: 0.1476 - val_mean_absolute_error: 0.1476\n",
            "Epoch 229/500\n",
            "174/188 [==========================>...] - ETA: 0s - loss: 0.1424 - mean_absolute_error: 0.1424\n",
            "Epoch 00229: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1397 - mean_absolute_error: 0.1397 - val_loss: 0.1526 - val_mean_absolute_error: 0.1526\n",
            "Epoch 230/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1574 - mean_absolute_error: 0.1574\n",
            "Epoch 00230: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1584 - mean_absolute_error: 0.1584 - val_loss: 0.0616 - val_mean_absolute_error: 0.0616\n",
            "Epoch 231/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1195 - mean_absolute_error: 0.1195\n",
            "Epoch 00231: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1198 - mean_absolute_error: 0.1198 - val_loss: 0.2280 - val_mean_absolute_error: 0.2280\n",
            "Epoch 232/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1444 - mean_absolute_error: 0.1444\n",
            "Epoch 00232: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1429 - mean_absolute_error: 0.1429 - val_loss: 0.0901 - val_mean_absolute_error: 0.0901\n",
            "Epoch 233/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1852 - mean_absolute_error: 0.1852\n",
            "Epoch 00233: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1824 - mean_absolute_error: 0.1824 - val_loss: 0.1753 - val_mean_absolute_error: 0.1753\n",
            "Epoch 234/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1556 - mean_absolute_error: 0.1556\n",
            "Epoch 00234: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1541 - mean_absolute_error: 0.1541 - val_loss: 0.2291 - val_mean_absolute_error: 0.2291\n",
            "Epoch 235/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1347 - mean_absolute_error: 0.1347\n",
            "Epoch 00235: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1365 - mean_absolute_error: 0.1365 - val_loss: 0.0645 - val_mean_absolute_error: 0.0645\n",
            "Epoch 236/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1819 - mean_absolute_error: 0.1819\n",
            "Epoch 00236: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1804 - mean_absolute_error: 0.1804 - val_loss: 0.1740 - val_mean_absolute_error: 0.1740\n",
            "Epoch 237/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1726 - mean_absolute_error: 0.1726\n",
            "Epoch 00237: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1715 - mean_absolute_error: 0.1715 - val_loss: 0.0704 - val_mean_absolute_error: 0.0704\n",
            "Epoch 238/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.0865 - mean_absolute_error: 0.0865\n",
            "Epoch 00238: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0865 - mean_absolute_error: 0.0865 - val_loss: 0.1289 - val_mean_absolute_error: 0.1289\n",
            "Epoch 239/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1234 - mean_absolute_error: 0.1234\n",
            "Epoch 00239: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1243 - mean_absolute_error: 0.1243 - val_loss: 0.0542 - val_mean_absolute_error: 0.0542\n",
            "Epoch 240/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1034 - mean_absolute_error: 0.1034\n",
            "Epoch 00240: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1034 - mean_absolute_error: 0.1034 - val_loss: 0.2412 - val_mean_absolute_error: 0.2412\n",
            "Epoch 241/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1504 - mean_absolute_error: 0.1504\n",
            "Epoch 00241: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1507 - mean_absolute_error: 0.1507 - val_loss: 0.0524 - val_mean_absolute_error: 0.0524\n",
            "Epoch 242/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1479 - mean_absolute_error: 0.1479\n",
            "Epoch 00242: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1478 - mean_absolute_error: 0.1478 - val_loss: 0.0470 - val_mean_absolute_error: 0.0470\n",
            "Epoch 243/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1140 - mean_absolute_error: 0.1140\n",
            "Epoch 00243: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1140 - mean_absolute_error: 0.1140 - val_loss: 0.1137 - val_mean_absolute_error: 0.1137\n",
            "Epoch 244/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1285 - mean_absolute_error: 0.1285\n",
            "Epoch 00244: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1284 - mean_absolute_error: 0.1284 - val_loss: 0.1845 - val_mean_absolute_error: 0.1845\n",
            "Epoch 245/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1489 - mean_absolute_error: 0.1489\n",
            "Epoch 00245: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1489 - mean_absolute_error: 0.1489 - val_loss: 0.2131 - val_mean_absolute_error: 0.2131\n",
            "Epoch 246/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.2838 - mean_absolute_error: 0.2838\n",
            "Epoch 00246: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2808 - mean_absolute_error: 0.2808 - val_loss: 0.1723 - val_mean_absolute_error: 0.1723\n",
            "Epoch 247/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1840 - mean_absolute_error: 0.1840\n",
            "Epoch 00247: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1845 - mean_absolute_error: 0.1845 - val_loss: 0.1731 - val_mean_absolute_error: 0.1731\n",
            "Epoch 248/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1149 - mean_absolute_error: 0.1149\n",
            "Epoch 00248: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1154 - mean_absolute_error: 0.1154 - val_loss: 0.0716 - val_mean_absolute_error: 0.0716\n",
            "Epoch 249/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1698 - mean_absolute_error: 0.1698\n",
            "Epoch 00249: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1680 - mean_absolute_error: 0.1680 - val_loss: 0.1430 - val_mean_absolute_error: 0.1430\n",
            "Epoch 250/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1451 - mean_absolute_error: 0.1451\n",
            "Epoch 00250: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1438 - mean_absolute_error: 0.1438 - val_loss: 0.0534 - val_mean_absolute_error: 0.0534\n",
            "Epoch 251/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.1402 - mean_absolute_error: 0.1402\n",
            "Epoch 00251: val_loss did not improve from 0.04656\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1559 - mean_absolute_error: 0.1559 - val_loss: 0.2549 - val_mean_absolute_error: 0.2549\n",
            "Epoch 252/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1491 - mean_absolute_error: 0.1491\n",
            "Epoch 00252: val_loss improved from 0.04656 to 0.04308, saving model to Weights-252--0.04308.hdf5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1464 - mean_absolute_error: 0.1464 - val_loss: 0.0431 - val_mean_absolute_error: 0.0431\n",
            "Epoch 253/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1782 - mean_absolute_error: 0.1782\n",
            "Epoch 00253: val_loss did not improve from 0.04308\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1747 - mean_absolute_error: 0.1747 - val_loss: 0.1237 - val_mean_absolute_error: 0.1237\n",
            "Epoch 254/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1373 - mean_absolute_error: 0.1373\n",
            "Epoch 00254: val_loss did not improve from 0.04308\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1361 - mean_absolute_error: 0.1361 - val_loss: 0.1243 - val_mean_absolute_error: 0.1243\n",
            "Epoch 255/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1064 - mean_absolute_error: 0.1064\n",
            "Epoch 00255: val_loss improved from 0.04308 to 0.03593, saving model to Weights-255--0.03593.hdf5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1041 - mean_absolute_error: 0.1041 - val_loss: 0.0359 - val_mean_absolute_error: 0.0359\n",
            "Epoch 256/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0897 - mean_absolute_error: 0.0897\n",
            "Epoch 00256: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0907 - mean_absolute_error: 0.0907 - val_loss: 0.1417 - val_mean_absolute_error: 0.1417\n",
            "Epoch 257/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1962 - mean_absolute_error: 0.1962\n",
            "Epoch 00257: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1896 - mean_absolute_error: 0.1896 - val_loss: 0.0702 - val_mean_absolute_error: 0.0702\n",
            "Epoch 258/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.0825 - mean_absolute_error: 0.0825\n",
            "Epoch 00258: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0844 - mean_absolute_error: 0.0844 - val_loss: 0.0747 - val_mean_absolute_error: 0.0747\n",
            "Epoch 259/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1456 - mean_absolute_error: 0.1456\n",
            "Epoch 00259: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
            "Epoch 260/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1880 - mean_absolute_error: 0.1880\n",
            "Epoch 00260: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1894 - mean_absolute_error: 0.1894 - val_loss: 0.1550 - val_mean_absolute_error: 0.1550\n",
            "Epoch 261/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1461 - mean_absolute_error: 0.1461\n",
            "Epoch 00261: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1461 - mean_absolute_error: 0.1461 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
            "Epoch 262/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1052 - mean_absolute_error: 0.1052\n",
            "Epoch 00262: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1052 - mean_absolute_error: 0.1052 - val_loss: 0.1346 - val_mean_absolute_error: 0.1346\n",
            "Epoch 263/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1869 - mean_absolute_error: 0.1869\n",
            "Epoch 00263: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1869 - mean_absolute_error: 0.1869 - val_loss: 0.1090 - val_mean_absolute_error: 0.1090\n",
            "Epoch 264/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1229 - mean_absolute_error: 0.1229\n",
            "Epoch 00264: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1212 - mean_absolute_error: 0.1212 - val_loss: 0.0399 - val_mean_absolute_error: 0.0399\n",
            "Epoch 265/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1123 - mean_absolute_error: 0.1123\n",
            "Epoch 00265: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1116 - mean_absolute_error: 0.1116 - val_loss: 0.1233 - val_mean_absolute_error: 0.1233\n",
            "Epoch 266/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1147 - mean_absolute_error: 0.1147\n",
            "Epoch 00266: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1150 - mean_absolute_error: 0.1150 - val_loss: 0.1201 - val_mean_absolute_error: 0.1201\n",
            "Epoch 267/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1248 - mean_absolute_error: 0.1248\n",
            "Epoch 00267: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1246 - mean_absolute_error: 0.1246 - val_loss: 0.0525 - val_mean_absolute_error: 0.0525\n",
            "Epoch 268/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1221 - mean_absolute_error: 0.1221\n",
            "Epoch 00268: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.1378 - val_mean_absolute_error: 0.1378\n",
            "Epoch 269/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1102 - mean_absolute_error: 0.1102\n",
            "Epoch 00269: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1105 - mean_absolute_error: 0.1105 - val_loss: 0.1259 - val_mean_absolute_error: 0.1259\n",
            "Epoch 270/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1299 - mean_absolute_error: 0.1299\n",
            "Epoch 00270: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1300 - mean_absolute_error: 0.1300 - val_loss: 0.0594 - val_mean_absolute_error: 0.0594\n",
            "Epoch 271/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.2097 - mean_absolute_error: 0.2097\n",
            "Epoch 00271: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2043 - mean_absolute_error: 0.2043 - val_loss: 0.2411 - val_mean_absolute_error: 0.2411\n",
            "Epoch 272/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1955 - mean_absolute_error: 0.1955\n",
            "Epoch 00272: val_loss did not improve from 0.03593\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1964 - mean_absolute_error: 0.1964 - val_loss: 0.0503 - val_mean_absolute_error: 0.0503\n",
            "Epoch 273/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0994 - mean_absolute_error: 0.0994\n",
            "Epoch 00273: val_loss improved from 0.03593 to 0.03535, saving model to Weights-273--0.03535.hdf5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0983 - mean_absolute_error: 0.0983 - val_loss: 0.0353 - val_mean_absolute_error: 0.0353\n",
            "Epoch 274/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.0937 - mean_absolute_error: 0.0937\n",
            "Epoch 00274: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0937 - mean_absolute_error: 0.0937 - val_loss: 0.1237 - val_mean_absolute_error: 0.1237\n",
            "Epoch 275/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.0858 - mean_absolute_error: 0.0858\n",
            "Epoch 00275: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0858 - mean_absolute_error: 0.0858 - val_loss: 0.1265 - val_mean_absolute_error: 0.1265\n",
            "Epoch 276/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0832 - mean_absolute_error: 0.0832\n",
            "Epoch 00276: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0836 - mean_absolute_error: 0.0836 - val_loss: 0.0664 - val_mean_absolute_error: 0.0664\n",
            "Epoch 277/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1164 - mean_absolute_error: 0.1164\n",
            "Epoch 00277: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1171 - mean_absolute_error: 0.1171 - val_loss: 0.2641 - val_mean_absolute_error: 0.2641\n",
            "Epoch 278/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1259 - mean_absolute_error: 0.1259\n",
            "Epoch 00278: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1262 - mean_absolute_error: 0.1262 - val_loss: 0.1182 - val_mean_absolute_error: 0.1182\n",
            "Epoch 279/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1306 - mean_absolute_error: 0.1306\n",
            "Epoch 00279: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1305 - mean_absolute_error: 0.1305 - val_loss: 0.3125 - val_mean_absolute_error: 0.3125\n",
            "Epoch 280/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1308 - mean_absolute_error: 0.1308\n",
            "Epoch 00280: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1313 - mean_absolute_error: 0.1313 - val_loss: 0.0693 - val_mean_absolute_error: 0.0693\n",
            "Epoch 281/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1597 - mean_absolute_error: 0.1597\n",
            "Epoch 00281: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1600 - mean_absolute_error: 0.1600 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
            "Epoch 282/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1367 - mean_absolute_error: 0.1367\n",
            "Epoch 00282: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1337 - mean_absolute_error: 0.1337 - val_loss: 0.0605 - val_mean_absolute_error: 0.0605\n",
            "Epoch 283/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1111 - mean_absolute_error: 0.1111\n",
            "Epoch 00283: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1148 - mean_absolute_error: 0.1148 - val_loss: 0.2221 - val_mean_absolute_error: 0.2221\n",
            "Epoch 284/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1831 - mean_absolute_error: 0.1831\n",
            "Epoch 00284: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1812 - mean_absolute_error: 0.1812 - val_loss: 0.0874 - val_mean_absolute_error: 0.0874\n",
            "Epoch 285/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1068 - mean_absolute_error: 0.1068\n",
            "Epoch 00285: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1067 - mean_absolute_error: 0.1067 - val_loss: 0.0674 - val_mean_absolute_error: 0.0674\n",
            "Epoch 286/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1510 - mean_absolute_error: 0.1510\n",
            "Epoch 00286: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1502 - mean_absolute_error: 0.1502 - val_loss: 0.0612 - val_mean_absolute_error: 0.0612\n",
            "Epoch 287/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1053 - mean_absolute_error: 0.1053\n",
            "Epoch 00287: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1047 - mean_absolute_error: 0.1047 - val_loss: 0.0478 - val_mean_absolute_error: 0.0478\n",
            "Epoch 288/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1520 - mean_absolute_error: 0.1520\n",
            "Epoch 00288: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1523 - mean_absolute_error: 0.1523 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
            "Epoch 289/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1429 - mean_absolute_error: 0.1429\n",
            "Epoch 00289: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1447 - mean_absolute_error: 0.1447 - val_loss: 0.1958 - val_mean_absolute_error: 0.1958\n",
            "Epoch 290/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1404 - mean_absolute_error: 0.1404\n",
            "Epoch 00290: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1399 - mean_absolute_error: 0.1399 - val_loss: 0.1347 - val_mean_absolute_error: 0.1347\n",
            "Epoch 291/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.0831 - mean_absolute_error: 0.0831\n",
            "Epoch 00291: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0825 - mean_absolute_error: 0.0825 - val_loss: 0.0826 - val_mean_absolute_error: 0.0826\n",
            "Epoch 292/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1002 - mean_absolute_error: 0.1002\n",
            "Epoch 00292: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1041 - mean_absolute_error: 0.1041 - val_loss: 0.1097 - val_mean_absolute_error: 0.1097\n",
            "Epoch 293/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0915 - mean_absolute_error: 0.0915\n",
            "Epoch 00293: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0928 - mean_absolute_error: 0.0928 - val_loss: 0.1161 - val_mean_absolute_error: 0.1161\n",
            "Epoch 294/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1061 - mean_absolute_error: 0.1061\n",
            "Epoch 00294: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1080 - mean_absolute_error: 0.1080 - val_loss: 0.0545 - val_mean_absolute_error: 0.0545\n",
            "Epoch 295/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1033 - mean_absolute_error: 0.1033\n",
            "Epoch 00295: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1032 - mean_absolute_error: 0.1032 - val_loss: 0.2201 - val_mean_absolute_error: 0.2201\n",
            "Epoch 296/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0993 - mean_absolute_error: 0.0993\n",
            "Epoch 00296: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1027 - mean_absolute_error: 0.1027 - val_loss: 0.0741 - val_mean_absolute_error: 0.0741\n",
            "Epoch 297/500\n",
            "175/188 [==========================>...] - ETA: 0s - loss: 0.1243 - mean_absolute_error: 0.1243\n",
            "Epoch 00297: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1244 - mean_absolute_error: 0.1244 - val_loss: 0.1459 - val_mean_absolute_error: 0.1459\n",
            "Epoch 298/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2333 - mean_absolute_error: 0.2333\n",
            "Epoch 00298: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2368 - mean_absolute_error: 0.2368 - val_loss: 0.1949 - val_mean_absolute_error: 0.1949\n",
            "Epoch 299/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1225 - mean_absolute_error: 0.1225\n",
            "Epoch 00299: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1216 - mean_absolute_error: 0.1216 - val_loss: 0.1943 - val_mean_absolute_error: 0.1943\n",
            "Epoch 300/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1039 - mean_absolute_error: 0.1039\n",
            "Epoch 00300: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1051 - mean_absolute_error: 0.1051 - val_loss: 0.0688 - val_mean_absolute_error: 0.0688\n",
            "Epoch 301/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1322 - mean_absolute_error: 0.1322\n",
            "Epoch 00301: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1311 - mean_absolute_error: 0.1311 - val_loss: 0.2858 - val_mean_absolute_error: 0.2858\n",
            "Epoch 302/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1092 - mean_absolute_error: 0.1092\n",
            "Epoch 00302: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1080 - mean_absolute_error: 0.1080 - val_loss: 0.1953 - val_mean_absolute_error: 0.1953\n",
            "Epoch 303/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1042 - mean_absolute_error: 0.1042\n",
            "Epoch 00303: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1048 - mean_absolute_error: 0.1048 - val_loss: 0.2138 - val_mean_absolute_error: 0.2138\n",
            "Epoch 304/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1562 - mean_absolute_error: 0.1562\n",
            "Epoch 00304: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1537 - mean_absolute_error: 0.1537 - val_loss: 0.1358 - val_mean_absolute_error: 0.1358\n",
            "Epoch 305/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1156 - mean_absolute_error: 0.1156\n",
            "Epoch 00305: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1169 - mean_absolute_error: 0.1169 - val_loss: 0.0482 - val_mean_absolute_error: 0.0482\n",
            "Epoch 306/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1384 - mean_absolute_error: 0.1384\n",
            "Epoch 00306: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1375 - mean_absolute_error: 0.1375 - val_loss: 0.1223 - val_mean_absolute_error: 0.1223\n",
            "Epoch 307/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1309 - mean_absolute_error: 0.1309\n",
            "Epoch 00307: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1321 - mean_absolute_error: 0.1321 - val_loss: 0.2067 - val_mean_absolute_error: 0.2067\n",
            "Epoch 308/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1810 - mean_absolute_error: 0.1810\n",
            "Epoch 00308: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1775 - mean_absolute_error: 0.1775 - val_loss: 0.1228 - val_mean_absolute_error: 0.1228\n",
            "Epoch 309/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1121 - mean_absolute_error: 0.1121\n",
            "Epoch 00309: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1117 - mean_absolute_error: 0.1117 - val_loss: 0.0954 - val_mean_absolute_error: 0.0954\n",
            "Epoch 310/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1332 - mean_absolute_error: 0.1332\n",
            "Epoch 00310: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1323 - mean_absolute_error: 0.1323 - val_loss: 0.0705 - val_mean_absolute_error: 0.0705\n",
            "Epoch 311/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0961 - mean_absolute_error: 0.0961\n",
            "Epoch 00311: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0988 - mean_absolute_error: 0.0988 - val_loss: 0.4200 - val_mean_absolute_error: 0.4200\n",
            "Epoch 312/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1564 - mean_absolute_error: 0.1564\n",
            "Epoch 00312: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1556 - mean_absolute_error: 0.1556 - val_loss: 0.1467 - val_mean_absolute_error: 0.1467\n",
            "Epoch 313/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1097 - mean_absolute_error: 0.1097\n",
            "Epoch 00313: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1107 - mean_absolute_error: 0.1107 - val_loss: 0.0902 - val_mean_absolute_error: 0.0902\n",
            "Epoch 314/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1234 - mean_absolute_error: 0.1234\n",
            "Epoch 00314: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1234 - mean_absolute_error: 0.1234 - val_loss: 0.2086 - val_mean_absolute_error: 0.2086\n",
            "Epoch 315/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1049 - mean_absolute_error: 0.1049\n",
            "Epoch 00315: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1049 - mean_absolute_error: 0.1049 - val_loss: 0.1171 - val_mean_absolute_error: 0.1171\n",
            "Epoch 316/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.0996 - mean_absolute_error: 0.0996\n",
            "Epoch 00316: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0996 - mean_absolute_error: 0.0996 - val_loss: 0.1507 - val_mean_absolute_error: 0.1507\n",
            "Epoch 317/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1231 - mean_absolute_error: 0.1231\n",
            "Epoch 00317: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1244 - mean_absolute_error: 0.1244 - val_loss: 0.2039 - val_mean_absolute_error: 0.2039\n",
            "Epoch 318/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.2323 - mean_absolute_error: 0.2323\n",
            "Epoch 00318: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2892 - val_mean_absolute_error: 0.2892\n",
            "Epoch 319/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1497 - mean_absolute_error: 0.1497\n",
            "Epoch 00319: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1487 - mean_absolute_error: 0.1487 - val_loss: 0.1993 - val_mean_absolute_error: 0.1993\n",
            "Epoch 320/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1322 - mean_absolute_error: 0.1322\n",
            "Epoch 00320: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1306 - mean_absolute_error: 0.1306 - val_loss: 0.0931 - val_mean_absolute_error: 0.0931\n",
            "Epoch 321/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1222 - mean_absolute_error: 0.1222\n",
            "Epoch 00321: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1222 - mean_absolute_error: 0.1222 - val_loss: 0.1943 - val_mean_absolute_error: 0.1943\n",
            "Epoch 322/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1776 - mean_absolute_error: 0.1776\n",
            "Epoch 00322: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1776 - mean_absolute_error: 0.1776 - val_loss: 0.0808 - val_mean_absolute_error: 0.0808\n",
            "Epoch 323/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1101 - mean_absolute_error: 0.1101\n",
            "Epoch 00323: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1097 - mean_absolute_error: 0.1097 - val_loss: 0.1322 - val_mean_absolute_error: 0.1322\n",
            "Epoch 324/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1282 - mean_absolute_error: 0.1282\n",
            "Epoch 00324: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1275 - mean_absolute_error: 0.1275 - val_loss: 0.0880 - val_mean_absolute_error: 0.0880\n",
            "Epoch 325/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1055 - mean_absolute_error: 0.1055\n",
            "Epoch 00325: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1054 - mean_absolute_error: 0.1054 - val_loss: 0.1071 - val_mean_absolute_error: 0.1071\n",
            "Epoch 326/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1280 - mean_absolute_error: 0.1280\n",
            "Epoch 00326: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1246 - mean_absolute_error: 0.1246 - val_loss: 0.0502 - val_mean_absolute_error: 0.0502\n",
            "Epoch 327/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0874 - mean_absolute_error: 0.0874\n",
            "Epoch 00327: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0872 - mean_absolute_error: 0.0872 - val_loss: 0.1409 - val_mean_absolute_error: 0.1409\n",
            "Epoch 328/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1029 - mean_absolute_error: 0.1029\n",
            "Epoch 00328: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1079 - mean_absolute_error: 0.1079 - val_loss: 0.1965 - val_mean_absolute_error: 0.1965\n",
            "Epoch 329/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1017 - mean_absolute_error: 0.1017\n",
            "Epoch 00329: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1011 - mean_absolute_error: 0.1011 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
            "Epoch 330/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0919 - mean_absolute_error: 0.0919\n",
            "Epoch 00330: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0940 - mean_absolute_error: 0.0940 - val_loss: 0.0730 - val_mean_absolute_error: 0.0730\n",
            "Epoch 331/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1114 - mean_absolute_error: 0.1114\n",
            "Epoch 00331: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1080 - mean_absolute_error: 0.1080 - val_loss: 0.1423 - val_mean_absolute_error: 0.1423\n",
            "Epoch 332/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1198 - mean_absolute_error: 0.1198\n",
            "Epoch 00332: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1186 - mean_absolute_error: 0.1186 - val_loss: 0.0388 - val_mean_absolute_error: 0.0388\n",
            "Epoch 333/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.0838 - mean_absolute_error: 0.0838\n",
            "Epoch 00333: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0838 - mean_absolute_error: 0.0838 - val_loss: 0.2377 - val_mean_absolute_error: 0.2377\n",
            "Epoch 334/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1094 - mean_absolute_error: 0.1094\n",
            "Epoch 00334: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1088 - mean_absolute_error: 0.1088 - val_loss: 0.1209 - val_mean_absolute_error: 0.1209\n",
            "Epoch 335/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0679 - mean_absolute_error: 0.0679\n",
            "Epoch 00335: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0686 - mean_absolute_error: 0.0686 - val_loss: 0.0795 - val_mean_absolute_error: 0.0795\n",
            "Epoch 336/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1114 - mean_absolute_error: 0.1114\n",
            "Epoch 00336: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1157 - mean_absolute_error: 0.1157 - val_loss: 0.1702 - val_mean_absolute_error: 0.1702\n",
            "Epoch 337/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1456 - mean_absolute_error: 0.1456\n",
            "Epoch 00337: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1426 - mean_absolute_error: 0.1426 - val_loss: 0.0498 - val_mean_absolute_error: 0.0498\n",
            "Epoch 338/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1027 - mean_absolute_error: 0.1027\n",
            "Epoch 00338: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1027 - mean_absolute_error: 0.1027 - val_loss: 0.2284 - val_mean_absolute_error: 0.2284\n",
            "Epoch 339/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1251 - mean_absolute_error: 0.1251\n",
            "Epoch 00339: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1240 - mean_absolute_error: 0.1240 - val_loss: 0.0430 - val_mean_absolute_error: 0.0430\n",
            "Epoch 340/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1012 - mean_absolute_error: 0.1012\n",
            "Epoch 00340: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0998 - mean_absolute_error: 0.0998 - val_loss: 0.1083 - val_mean_absolute_error: 0.1083\n",
            "Epoch 341/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1130 - mean_absolute_error: 0.1130\n",
            "Epoch 00341: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1117 - mean_absolute_error: 0.1117 - val_loss: 0.1760 - val_mean_absolute_error: 0.1760\n",
            "Epoch 342/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1006 - mean_absolute_error: 0.1006\n",
            "Epoch 00342: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0990 - mean_absolute_error: 0.0990 - val_loss: 0.0401 - val_mean_absolute_error: 0.0401\n",
            "Epoch 343/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1040 - mean_absolute_error: 0.1040\n",
            "Epoch 00343: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1036 - mean_absolute_error: 0.1036 - val_loss: 0.0519 - val_mean_absolute_error: 0.0519\n",
            "Epoch 344/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.0973\n",
            "Epoch 00344: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0991 - mean_absolute_error: 0.0991 - val_loss: 0.1431 - val_mean_absolute_error: 0.1431\n",
            "Epoch 345/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1409 - mean_absolute_error: 0.1409\n",
            "Epoch 00345: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1403 - mean_absolute_error: 0.1403 - val_loss: 0.4075 - val_mean_absolute_error: 0.4075\n",
            "Epoch 346/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1321 - mean_absolute_error: 0.1321\n",
            "Epoch 00346: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1313 - mean_absolute_error: 0.1313 - val_loss: 0.1702 - val_mean_absolute_error: 0.1702\n",
            "Epoch 347/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1458 - mean_absolute_error: 0.1458\n",
            "Epoch 00347: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1412 - mean_absolute_error: 0.1412 - val_loss: 0.0533 - val_mean_absolute_error: 0.0533\n",
            "Epoch 348/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0989 - mean_absolute_error: 0.0989\n",
            "Epoch 00348: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0985 - mean_absolute_error: 0.0985 - val_loss: 0.2369 - val_mean_absolute_error: 0.2369\n",
            "Epoch 349/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1441 - mean_absolute_error: 0.1441\n",
            "Epoch 00349: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1449 - mean_absolute_error: 0.1449 - val_loss: 0.0677 - val_mean_absolute_error: 0.0677\n",
            "Epoch 350/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.0767 - mean_absolute_error: 0.0767\n",
            "Epoch 00350: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0775 - mean_absolute_error: 0.0775 - val_loss: 0.2620 - val_mean_absolute_error: 0.2620\n",
            "Epoch 351/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.0718 - mean_absolute_error: 0.0718\n",
            "Epoch 00351: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0737 - mean_absolute_error: 0.0737 - val_loss: 0.1286 - val_mean_absolute_error: 0.1286\n",
            "Epoch 352/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1081 - mean_absolute_error: 0.1081\n",
            "Epoch 00352: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1147 - mean_absolute_error: 0.1147 - val_loss: 0.3287 - val_mean_absolute_error: 0.3287\n",
            "Epoch 353/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1245 - mean_absolute_error: 0.1245\n",
            "Epoch 00353: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1218 - mean_absolute_error: 0.1218 - val_loss: 0.0528 - val_mean_absolute_error: 0.0528\n",
            "Epoch 354/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0744 - mean_absolute_error: 0.0744\n",
            "Epoch 00354: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0826 - mean_absolute_error: 0.0826 - val_loss: 0.1358 - val_mean_absolute_error: 0.1358\n",
            "Epoch 355/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1344 - mean_absolute_error: 0.1344\n",
            "Epoch 00355: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1332 - mean_absolute_error: 0.1332 - val_loss: 0.0609 - val_mean_absolute_error: 0.0609\n",
            "Epoch 356/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0884 - mean_absolute_error: 0.0884\n",
            "Epoch 00356: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0876 - mean_absolute_error: 0.0876 - val_loss: 0.0534 - val_mean_absolute_error: 0.0534\n",
            "Epoch 357/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0981 - mean_absolute_error: 0.0981\n",
            "Epoch 00357: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0980 - mean_absolute_error: 0.0980 - val_loss: 0.0444 - val_mean_absolute_error: 0.0444\n",
            "Epoch 358/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1440 - mean_absolute_error: 0.1440\n",
            "Epoch 00358: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1434 - mean_absolute_error: 0.1434 - val_loss: 0.2292 - val_mean_absolute_error: 0.2292\n",
            "Epoch 359/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.0985 - mean_absolute_error: 0.0985\n",
            "Epoch 00359: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0983 - mean_absolute_error: 0.0983 - val_loss: 0.1055 - val_mean_absolute_error: 0.1055\n",
            "Epoch 360/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1779 - mean_absolute_error: 0.1779\n",
            "Epoch 00360: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1736 - mean_absolute_error: 0.1736 - val_loss: 0.1124 - val_mean_absolute_error: 0.1124\n",
            "Epoch 361/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.0997 - mean_absolute_error: 0.0997\n",
            "Epoch 00361: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0978 - mean_absolute_error: 0.0978 - val_loss: 0.0370 - val_mean_absolute_error: 0.0370\n",
            "Epoch 362/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1095 - mean_absolute_error: 0.1095\n",
            "Epoch 00362: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1090 - mean_absolute_error: 0.1090 - val_loss: 0.1408 - val_mean_absolute_error: 0.1408\n",
            "Epoch 363/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1225 - mean_absolute_error: 0.1225\n",
            "Epoch 00363: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1257 - mean_absolute_error: 0.1257 - val_loss: 0.1908 - val_mean_absolute_error: 0.1908\n",
            "Epoch 364/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1374 - mean_absolute_error: 0.1374\n",
            "Epoch 00364: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1373 - mean_absolute_error: 0.1373 - val_loss: 0.1406 - val_mean_absolute_error: 0.1406\n",
            "Epoch 365/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1720 - mean_absolute_error: 0.1720\n",
            "Epoch 00365: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1754 - mean_absolute_error: 0.1754 - val_loss: 0.1894 - val_mean_absolute_error: 0.1894\n",
            "Epoch 366/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1012 - mean_absolute_error: 0.1012\n",
            "Epoch 00366: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1005 - mean_absolute_error: 0.1005 - val_loss: 0.1576 - val_mean_absolute_error: 0.1576\n",
            "Epoch 367/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1215 - mean_absolute_error: 0.1215\n",
            "Epoch 00367: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1215 - mean_absolute_error: 0.1215 - val_loss: 0.2365 - val_mean_absolute_error: 0.2365\n",
            "Epoch 368/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1291 - mean_absolute_error: 0.1291\n",
            "Epoch 00368: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1278 - mean_absolute_error: 0.1278 - val_loss: 0.0542 - val_mean_absolute_error: 0.0542\n",
            "Epoch 369/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1556 - mean_absolute_error: 0.1556\n",
            "Epoch 00369: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1556 - mean_absolute_error: 0.1556 - val_loss: 0.1677 - val_mean_absolute_error: 0.1677\n",
            "Epoch 370/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1104 - mean_absolute_error: 0.1104\n",
            "Epoch 00370: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1105 - mean_absolute_error: 0.1105 - val_loss: 0.0704 - val_mean_absolute_error: 0.0704\n",
            "Epoch 371/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1491 - mean_absolute_error: 0.1491\n",
            "Epoch 00371: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1472 - mean_absolute_error: 0.1472 - val_loss: 0.0995 - val_mean_absolute_error: 0.0995\n",
            "Epoch 372/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1272 - mean_absolute_error: 0.1272\n",
            "Epoch 00372: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1272 - mean_absolute_error: 0.1272 - val_loss: 0.1033 - val_mean_absolute_error: 0.1033\n",
            "Epoch 373/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0932 - mean_absolute_error: 0.0932\n",
            "Epoch 00373: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0911 - mean_absolute_error: 0.0911 - val_loss: 0.1186 - val_mean_absolute_error: 0.1186\n",
            "Epoch 374/500\n",
            "176/188 [===========================>..] - ETA: 0s - loss: 0.1902 - mean_absolute_error: 0.1902\n",
            "Epoch 00374: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2053 - mean_absolute_error: 0.2053 - val_loss: 0.5221 - val_mean_absolute_error: 0.5221\n",
            "Epoch 375/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1303 - mean_absolute_error: 0.1303\n",
            "Epoch 00375: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1310 - mean_absolute_error: 0.1310 - val_loss: 0.1230 - val_mean_absolute_error: 0.1230\n",
            "Epoch 376/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1208 - mean_absolute_error: 0.1208\n",
            "Epoch 00376: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1192 - mean_absolute_error: 0.1192 - val_loss: 0.1009 - val_mean_absolute_error: 0.1009\n",
            "Epoch 377/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1323 - mean_absolute_error: 0.1323\n",
            "Epoch 00377: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1289 - mean_absolute_error: 0.1289 - val_loss: 0.0500 - val_mean_absolute_error: 0.0500\n",
            "Epoch 378/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1008 - mean_absolute_error: 0.1008\n",
            "Epoch 00378: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1004 - mean_absolute_error: 0.1004 - val_loss: 0.0867 - val_mean_absolute_error: 0.0867\n",
            "Epoch 379/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0905 - mean_absolute_error: 0.0905\n",
            "Epoch 00379: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0904 - mean_absolute_error: 0.0904 - val_loss: 0.0764 - val_mean_absolute_error: 0.0764\n",
            "Epoch 380/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1296 - mean_absolute_error: 0.1296\n",
            "Epoch 00380: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1246 - mean_absolute_error: 0.1246 - val_loss: 0.0483 - val_mean_absolute_error: 0.0483\n",
            "Epoch 381/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0734 - mean_absolute_error: 0.0734\n",
            "Epoch 00381: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0730 - mean_absolute_error: 0.0730 - val_loss: 0.2015 - val_mean_absolute_error: 0.2015\n",
            "Epoch 382/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1246 - mean_absolute_error: 0.1246\n",
            "Epoch 00382: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1240 - mean_absolute_error: 0.1240 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
            "Epoch 383/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1212 - mean_absolute_error: 0.1212\n",
            "Epoch 00383: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1211 - mean_absolute_error: 0.1211 - val_loss: 0.2888 - val_mean_absolute_error: 0.2888\n",
            "Epoch 384/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1034 - mean_absolute_error: 0.1034\n",
            "Epoch 00384: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1055 - mean_absolute_error: 0.1055 - val_loss: 0.1458 - val_mean_absolute_error: 0.1458\n",
            "Epoch 385/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1125 - mean_absolute_error: 0.1125\n",
            "Epoch 00385: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1118 - mean_absolute_error: 0.1118 - val_loss: 0.2338 - val_mean_absolute_error: 0.2338\n",
            "Epoch 386/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1052 - mean_absolute_error: 0.1052\n",
            "Epoch 00386: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1061 - mean_absolute_error: 0.1061 - val_loss: 0.1818 - val_mean_absolute_error: 0.1818\n",
            "Epoch 387/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1051 - mean_absolute_error: 0.1051\n",
            "Epoch 00387: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1047 - mean_absolute_error: 0.1047 - val_loss: 0.0888 - val_mean_absolute_error: 0.0888\n",
            "Epoch 388/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1279 - mean_absolute_error: 0.1279\n",
            "Epoch 00388: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1278 - mean_absolute_error: 0.1278 - val_loss: 0.3402 - val_mean_absolute_error: 0.3402\n",
            "Epoch 389/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.0702 - mean_absolute_error: 0.0702\n",
            "Epoch 00389: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0709 - mean_absolute_error: 0.0709 - val_loss: 0.1907 - val_mean_absolute_error: 0.1907\n",
            "Epoch 390/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0909 - mean_absolute_error: 0.0909\n",
            "Epoch 00390: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0936 - mean_absolute_error: 0.0936 - val_loss: 0.1291 - val_mean_absolute_error: 0.1291\n",
            "Epoch 391/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1570 - mean_absolute_error: 0.1570\n",
            "Epoch 00391: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1583 - mean_absolute_error: 0.1583 - val_loss: 0.0601 - val_mean_absolute_error: 0.0601\n",
            "Epoch 392/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1044 - mean_absolute_error: 0.1044\n",
            "Epoch 00392: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1068 - mean_absolute_error: 0.1068 - val_loss: 0.0558 - val_mean_absolute_error: 0.0558\n",
            "Epoch 393/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0997 - mean_absolute_error: 0.0997\n",
            "Epoch 00393: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1022 - mean_absolute_error: 0.1022 - val_loss: 0.1277 - val_mean_absolute_error: 0.1277\n",
            "Epoch 394/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1051 - mean_absolute_error: 0.1051\n",
            "Epoch 00394: val_loss did not improve from 0.03535\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1046 - mean_absolute_error: 0.1046 - val_loss: 0.1438 - val_mean_absolute_error: 0.1438\n",
            "Epoch 395/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1008 - mean_absolute_error: 0.1008\n",
            "Epoch 00395: val_loss improved from 0.03535 to 0.02266, saving model to Weights-395--0.02266.hdf5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1005 - mean_absolute_error: 0.1005 - val_loss: 0.0227 - val_mean_absolute_error: 0.0227\n",
            "Epoch 396/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1036 - mean_absolute_error: 0.1036\n",
            "Epoch 00396: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1036 - mean_absolute_error: 0.1036 - val_loss: 0.0986 - val_mean_absolute_error: 0.0986\n",
            "Epoch 397/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1105 - mean_absolute_error: 0.1105\n",
            "Epoch 00397: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1099 - mean_absolute_error: 0.1099 - val_loss: 0.0919 - val_mean_absolute_error: 0.0919\n",
            "Epoch 398/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1712 - mean_absolute_error: 0.1712\n",
            "Epoch 00398: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1685 - mean_absolute_error: 0.1685 - val_loss: 0.1228 - val_mean_absolute_error: 0.1228\n",
            "Epoch 399/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1347 - mean_absolute_error: 0.1347\n",
            "Epoch 00399: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1339 - mean_absolute_error: 0.1339 - val_loss: 0.1510 - val_mean_absolute_error: 0.1510\n",
            "Epoch 400/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.1016 - mean_absolute_error: 0.1016\n",
            "Epoch 00400: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1015 - mean_absolute_error: 0.1015 - val_loss: 0.0346 - val_mean_absolute_error: 0.0346\n",
            "Epoch 401/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1099 - mean_absolute_error: 0.1099\n",
            "Epoch 00401: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1107 - mean_absolute_error: 0.1107 - val_loss: 0.2822 - val_mean_absolute_error: 0.2822\n",
            "Epoch 402/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1326 - mean_absolute_error: 0.1326\n",
            "Epoch 00402: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1364 - mean_absolute_error: 0.1364 - val_loss: 0.0635 - val_mean_absolute_error: 0.0635\n",
            "Epoch 403/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1176 - mean_absolute_error: 0.1176\n",
            "Epoch 00403: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1187 - mean_absolute_error: 0.1187 - val_loss: 0.1598 - val_mean_absolute_error: 0.1598\n",
            "Epoch 404/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1282 - mean_absolute_error: 0.1282\n",
            "Epoch 00404: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1306 - mean_absolute_error: 0.1306 - val_loss: 0.3579 - val_mean_absolute_error: 0.3579\n",
            "Epoch 405/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1002 - mean_absolute_error: 0.1002\n",
            "Epoch 00405: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1670 - val_mean_absolute_error: 0.1670\n",
            "Epoch 406/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1266 - mean_absolute_error: 0.1266\n",
            "Epoch 00406: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1270 - mean_absolute_error: 0.1270 - val_loss: 0.0474 - val_mean_absolute_error: 0.0474\n",
            "Epoch 407/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.0944 - mean_absolute_error: 0.0944\n",
            "Epoch 00407: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0964 - mean_absolute_error: 0.0964 - val_loss: 0.2812 - val_mean_absolute_error: 0.2812\n",
            "Epoch 408/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1039 - mean_absolute_error: 0.1039\n",
            "Epoch 00408: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1033 - mean_absolute_error: 0.1033 - val_loss: 0.0904 - val_mean_absolute_error: 0.0904\n",
            "Epoch 409/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1178 - mean_absolute_error: 0.1178\n",
            "Epoch 00409: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1154 - mean_absolute_error: 0.1154 - val_loss: 0.0243 - val_mean_absolute_error: 0.0243\n",
            "Epoch 410/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1242 - mean_absolute_error: 0.1242\n",
            "Epoch 00410: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1241 - mean_absolute_error: 0.1241 - val_loss: 0.1549 - val_mean_absolute_error: 0.1549\n",
            "Epoch 411/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1204 - mean_absolute_error: 0.1204\n",
            "Epoch 00411: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1309 - mean_absolute_error: 0.1309 - val_loss: 0.4072 - val_mean_absolute_error: 0.4072\n",
            "Epoch 412/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1371 - mean_absolute_error: 0.1371\n",
            "Epoch 00412: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1348 - mean_absolute_error: 0.1348 - val_loss: 0.0330 - val_mean_absolute_error: 0.0330\n",
            "Epoch 413/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1804 - mean_absolute_error: 0.1804\n",
            "Epoch 00413: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1792 - mean_absolute_error: 0.1792 - val_loss: 0.0946 - val_mean_absolute_error: 0.0946\n",
            "Epoch 414/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.0887 - mean_absolute_error: 0.0887\n",
            "Epoch 00414: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0904 - mean_absolute_error: 0.0904 - val_loss: 0.1121 - val_mean_absolute_error: 0.1121\n",
            "Epoch 415/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1275 - mean_absolute_error: 0.1275\n",
            "Epoch 00415: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1284 - mean_absolute_error: 0.1284 - val_loss: 0.0891 - val_mean_absolute_error: 0.0891\n",
            "Epoch 416/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.0794 - mean_absolute_error: 0.0794\n",
            "Epoch 00416: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0787 - mean_absolute_error: 0.0787 - val_loss: 0.0515 - val_mean_absolute_error: 0.0515\n",
            "Epoch 417/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1005 - mean_absolute_error: 0.1005\n",
            "Epoch 00417: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1004 - mean_absolute_error: 0.1004 - val_loss: 0.0392 - val_mean_absolute_error: 0.0392\n",
            "Epoch 418/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.0671 - mean_absolute_error: 0.0671\n",
            "Epoch 00418: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0702 - mean_absolute_error: 0.0702 - val_loss: 0.1297 - val_mean_absolute_error: 0.1297\n",
            "Epoch 419/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.0682 - mean_absolute_error: 0.0682\n",
            "Epoch 00419: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0679 - mean_absolute_error: 0.0679 - val_loss: 0.0259 - val_mean_absolute_error: 0.0259\n",
            "Epoch 420/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.0847 - mean_absolute_error: 0.0847\n",
            "Epoch 00420: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0848 - mean_absolute_error: 0.0848 - val_loss: 0.2105 - val_mean_absolute_error: 0.2105\n",
            "Epoch 421/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1504 - mean_absolute_error: 0.1504\n",
            "Epoch 00421: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1498 - mean_absolute_error: 0.1498 - val_loss: 0.2624 - val_mean_absolute_error: 0.2624\n",
            "Epoch 422/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.0969\n",
            "Epoch 00422: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.1036 - val_mean_absolute_error: 0.1036\n",
            "Epoch 423/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1009 - mean_absolute_error: 0.1009\n",
            "Epoch 00423: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1009 - mean_absolute_error: 0.1009 - val_loss: 0.1054 - val_mean_absolute_error: 0.1054\n",
            "Epoch 424/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1053 - mean_absolute_error: 0.1053\n",
            "Epoch 00424: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1043 - mean_absolute_error: 0.1043 - val_loss: 0.0795 - val_mean_absolute_error: 0.0795\n",
            "Epoch 425/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0792 - mean_absolute_error: 0.0792\n",
            "Epoch 00425: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0789 - mean_absolute_error: 0.0789 - val_loss: 0.1015 - val_mean_absolute_error: 0.1015\n",
            "Epoch 426/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.0847 - mean_absolute_error: 0.0847\n",
            "Epoch 00426: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0846 - mean_absolute_error: 0.0846 - val_loss: 0.0895 - val_mean_absolute_error: 0.0895\n",
            "Epoch 427/500\n",
            "177/188 [===========================>..] - ETA: 0s - loss: 0.1198 - mean_absolute_error: 0.1198\n",
            "Epoch 00427: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1165 - mean_absolute_error: 0.1165 - val_loss: 0.0499 - val_mean_absolute_error: 0.0499\n",
            "Epoch 428/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1509 - mean_absolute_error: 0.1509\n",
            "Epoch 00428: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1529 - mean_absolute_error: 0.1529 - val_loss: 0.2194 - val_mean_absolute_error: 0.2194\n",
            "Epoch 429/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1044 - mean_absolute_error: 0.1044\n",
            "Epoch 00429: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1021 - mean_absolute_error: 0.1021 - val_loss: 0.1236 - val_mean_absolute_error: 0.1236\n",
            "Epoch 430/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.0880 - mean_absolute_error: 0.0880\n",
            "Epoch 00430: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0880 - mean_absolute_error: 0.0880 - val_loss: 0.0587 - val_mean_absolute_error: 0.0587\n",
            "Epoch 431/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1127 - mean_absolute_error: 0.1127\n",
            "Epoch 00431: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1122 - mean_absolute_error: 0.1122 - val_loss: 0.3020 - val_mean_absolute_error: 0.3020\n",
            "Epoch 432/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1135 - mean_absolute_error: 0.1135\n",
            "Epoch 00432: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1128 - mean_absolute_error: 0.1128 - val_loss: 0.1986 - val_mean_absolute_error: 0.1986\n",
            "Epoch 433/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1020 - mean_absolute_error: 0.1020\n",
            "Epoch 00433: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1017 - mean_absolute_error: 0.1017 - val_loss: 0.1571 - val_mean_absolute_error: 0.1571\n",
            "Epoch 434/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1441 - mean_absolute_error: 0.1441\n",
            "Epoch 00434: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1441 - mean_absolute_error: 0.1441 - val_loss: 0.1136 - val_mean_absolute_error: 0.1136\n",
            "Epoch 435/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1258 - mean_absolute_error: 0.1258\n",
            "Epoch 00435: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1247 - mean_absolute_error: 0.1247 - val_loss: 0.0796 - val_mean_absolute_error: 0.0796\n",
            "Epoch 436/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1285 - mean_absolute_error: 0.1285\n",
            "Epoch 00436: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1265 - mean_absolute_error: 0.1265 - val_loss: 0.2980 - val_mean_absolute_error: 0.2980\n",
            "Epoch 437/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1322 - mean_absolute_error: 0.1322\n",
            "Epoch 00437: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1330 - mean_absolute_error: 0.1330 - val_loss: 0.0630 - val_mean_absolute_error: 0.0630\n",
            "Epoch 438/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1399 - mean_absolute_error: 0.1399\n",
            "Epoch 00438: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1392 - mean_absolute_error: 0.1392 - val_loss: 0.1705 - val_mean_absolute_error: 0.1705\n",
            "Epoch 439/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1557 - mean_absolute_error: 0.1557\n",
            "Epoch 00439: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1533 - mean_absolute_error: 0.1533 - val_loss: 0.1479 - val_mean_absolute_error: 0.1479\n",
            "Epoch 440/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0849 - mean_absolute_error: 0.0849\n",
            "Epoch 00440: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0842 - mean_absolute_error: 0.0842 - val_loss: 0.0751 - val_mean_absolute_error: 0.0751\n",
            "Epoch 441/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1066 - mean_absolute_error: 0.1066\n",
            "Epoch 00441: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1066 - mean_absolute_error: 0.1066 - val_loss: 0.0880 - val_mean_absolute_error: 0.0880\n",
            "Epoch 442/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1286 - mean_absolute_error: 0.1286\n",
            "Epoch 00442: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1261 - mean_absolute_error: 0.1261 - val_loss: 0.0587 - val_mean_absolute_error: 0.0587\n",
            "Epoch 443/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1084 - mean_absolute_error: 0.1084\n",
            "Epoch 00443: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1080 - mean_absolute_error: 0.1080 - val_loss: 0.0445 - val_mean_absolute_error: 0.0445\n",
            "Epoch 444/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.0720 - mean_absolute_error: 0.0720\n",
            "Epoch 00444: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0714 - mean_absolute_error: 0.0714 - val_loss: 0.1185 - val_mean_absolute_error: 0.1185\n",
            "Epoch 445/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0830 - mean_absolute_error: 0.0830\n",
            "Epoch 00445: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0828 - mean_absolute_error: 0.0828 - val_loss: 0.1040 - val_mean_absolute_error: 0.1040\n",
            "Epoch 446/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1217 - mean_absolute_error: 0.1217\n",
            "Epoch 00446: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1221 - mean_absolute_error: 0.1221 - val_loss: 0.0607 - val_mean_absolute_error: 0.0607\n",
            "Epoch 447/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0856 - mean_absolute_error: 0.0856\n",
            "Epoch 00447: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0868 - mean_absolute_error: 0.0868 - val_loss: 0.1065 - val_mean_absolute_error: 0.1065\n",
            "Epoch 448/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.0732 - mean_absolute_error: 0.0732\n",
            "Epoch 00448: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0732 - mean_absolute_error: 0.0732 - val_loss: 0.1248 - val_mean_absolute_error: 0.1248\n",
            "Epoch 449/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.0852 - mean_absolute_error: 0.0852\n",
            "Epoch 00449: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0843 - mean_absolute_error: 0.0843 - val_loss: 0.0983 - val_mean_absolute_error: 0.0983\n",
            "Epoch 450/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1174 - mean_absolute_error: 0.1174\n",
            "Epoch 00450: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1164 - mean_absolute_error: 0.1164 - val_loss: 0.0471 - val_mean_absolute_error: 0.0471\n",
            "Epoch 451/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1056 - mean_absolute_error: 0.1056\n",
            "Epoch 00451: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1064 - mean_absolute_error: 0.1064 - val_loss: 0.0555 - val_mean_absolute_error: 0.0555\n",
            "Epoch 452/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1058 - mean_absolute_error: 0.1058\n",
            "Epoch 00452: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1070 - mean_absolute_error: 0.1070 - val_loss: 0.3973 - val_mean_absolute_error: 0.3973\n",
            "Epoch 453/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1547 - mean_absolute_error: 0.1547\n",
            "Epoch 00453: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1538 - mean_absolute_error: 0.1538 - val_loss: 0.1387 - val_mean_absolute_error: 0.1387\n",
            "Epoch 454/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.1919 - mean_absolute_error: 0.1919\n",
            "Epoch 00454: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1893 - mean_absolute_error: 0.1893 - val_loss: 0.0946 - val_mean_absolute_error: 0.0946\n",
            "Epoch 455/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0988 - mean_absolute_error: 0.0988\n",
            "Epoch 00455: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0994 - mean_absolute_error: 0.0994 - val_loss: 0.1021 - val_mean_absolute_error: 0.1021\n",
            "Epoch 456/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1296 - mean_absolute_error: 0.1296\n",
            "Epoch 00456: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1267 - mean_absolute_error: 0.1267 - val_loss: 0.0706 - val_mean_absolute_error: 0.0706\n",
            "Epoch 457/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1135 - mean_absolute_error: 0.1135\n",
            "Epoch 00457: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1142 - mean_absolute_error: 0.1142 - val_loss: 0.0916 - val_mean_absolute_error: 0.0916\n",
            "Epoch 458/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0750 - mean_absolute_error: 0.0750\n",
            "Epoch 00458: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0748 - mean_absolute_error: 0.0748 - val_loss: 0.0391 - val_mean_absolute_error: 0.0391\n",
            "Epoch 459/500\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.0719 - mean_absolute_error: 0.0719\n",
            "Epoch 00459: val_loss did not improve from 0.02266\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0719 - mean_absolute_error: 0.0719 - val_loss: 0.0532 - val_mean_absolute_error: 0.0532\n",
            "Epoch 460/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.0688 - mean_absolute_error: 0.0688\n",
            "Epoch 00460: val_loss improved from 0.02266 to 0.02222, saving model to Weights-460--0.02222.hdf5\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0682 - mean_absolute_error: 0.0682 - val_loss: 0.0222 - val_mean_absolute_error: 0.0222\n",
            "Epoch 461/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.0921 - mean_absolute_error: 0.0921\n",
            "Epoch 00461: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0920 - mean_absolute_error: 0.0920 - val_loss: 0.0974 - val_mean_absolute_error: 0.0974\n",
            "Epoch 462/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.0936 - mean_absolute_error: 0.0936\n",
            "Epoch 00462: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0934 - mean_absolute_error: 0.0934 - val_loss: 0.0827 - val_mean_absolute_error: 0.0827\n",
            "Epoch 463/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.1324 - mean_absolute_error: 0.1324\n",
            "Epoch 00463: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1337 - mean_absolute_error: 0.1337 - val_loss: 0.0495 - val_mean_absolute_error: 0.0495\n",
            "Epoch 464/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1363 - mean_absolute_error: 0.1363\n",
            "Epoch 00464: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1350 - mean_absolute_error: 0.1350 - val_loss: 0.0727 - val_mean_absolute_error: 0.0727\n",
            "Epoch 465/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1092 - mean_absolute_error: 0.1092\n",
            "Epoch 00465: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1092 - mean_absolute_error: 0.1092 - val_loss: 0.0805 - val_mean_absolute_error: 0.0805\n",
            "Epoch 466/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.1057 - mean_absolute_error: 0.1057\n",
            "Epoch 00466: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1067 - mean_absolute_error: 0.1067 - val_loss: 0.0439 - val_mean_absolute_error: 0.0439\n",
            "Epoch 467/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1189 - mean_absolute_error: 0.1189\n",
            "Epoch 00467: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1186 - mean_absolute_error: 0.1186 - val_loss: 0.1322 - val_mean_absolute_error: 0.1322\n",
            "Epoch 468/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0812 - mean_absolute_error: 0.0812\n",
            "Epoch 00468: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0819 - mean_absolute_error: 0.0819 - val_loss: 0.1107 - val_mean_absolute_error: 0.1107\n",
            "Epoch 469/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1135 - mean_absolute_error: 0.1135\n",
            "Epoch 00469: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1119 - mean_absolute_error: 0.1119 - val_loss: 0.0732 - val_mean_absolute_error: 0.0732\n",
            "Epoch 470/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1137 - mean_absolute_error: 0.1137\n",
            "Epoch 00470: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1140 - mean_absolute_error: 0.1140 - val_loss: 0.1045 - val_mean_absolute_error: 0.1045\n",
            "Epoch 471/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1307 - mean_absolute_error: 0.1307\n",
            "Epoch 00471: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1307 - mean_absolute_error: 0.1307 - val_loss: 0.1297 - val_mean_absolute_error: 0.1297\n",
            "Epoch 472/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.0994 - mean_absolute_error: 0.0994\n",
            "Epoch 00472: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.0995 - mean_absolute_error: 0.0995 - val_loss: 0.0772 - val_mean_absolute_error: 0.0772\n",
            "Epoch 473/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0753 - mean_absolute_error: 0.0753\n",
            "Epoch 00473: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.0760 - mean_absolute_error: 0.0760 - val_loss: 0.0985 - val_mean_absolute_error: 0.0985\n",
            "Epoch 474/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1049 - mean_absolute_error: 0.1049\n",
            "Epoch 00474: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1049 - mean_absolute_error: 0.1049 - val_loss: 0.1587 - val_mean_absolute_error: 0.1587\n",
            "Epoch 475/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.0954 - mean_absolute_error: 0.0954\n",
            "Epoch 00475: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0957 - mean_absolute_error: 0.0957 - val_loss: 0.0555 - val_mean_absolute_error: 0.0555\n",
            "Epoch 476/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0956 - mean_absolute_error: 0.0956\n",
            "Epoch 00476: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0954 - mean_absolute_error: 0.0954 - val_loss: 0.1911 - val_mean_absolute_error: 0.1911\n",
            "Epoch 477/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.0951 - mean_absolute_error: 0.0951\n",
            "Epoch 00477: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0951 - mean_absolute_error: 0.0951 - val_loss: 0.0703 - val_mean_absolute_error: 0.0703\n",
            "Epoch 478/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.0747 - mean_absolute_error: 0.0747\n",
            "Epoch 00478: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0742 - mean_absolute_error: 0.0742 - val_loss: 0.0897 - val_mean_absolute_error: 0.0897\n",
            "Epoch 479/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0958 - mean_absolute_error: 0.0958\n",
            "Epoch 00479: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0958 - mean_absolute_error: 0.0958 - val_loss: 0.0914 - val_mean_absolute_error: 0.0914\n",
            "Epoch 480/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0995 - mean_absolute_error: 0.0995\n",
            "Epoch 00480: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1012 - mean_absolute_error: 0.1012 - val_loss: 0.1497 - val_mean_absolute_error: 0.1497\n",
            "Epoch 481/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.1610 - mean_absolute_error: 0.1610\n",
            "Epoch 00481: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1564 - mean_absolute_error: 0.1564 - val_loss: 0.0392 - val_mean_absolute_error: 0.0392\n",
            "Epoch 482/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.0829 - mean_absolute_error: 0.0829\n",
            "Epoch 00482: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0821 - mean_absolute_error: 0.0821 - val_loss: 0.0739 - val_mean_absolute_error: 0.0739\n",
            "Epoch 483/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.0819 - mean_absolute_error: 0.0819\n",
            "Epoch 00483: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0840 - mean_absolute_error: 0.0840 - val_loss: 0.1561 - val_mean_absolute_error: 0.1561\n",
            "Epoch 484/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.0970\n",
            "Epoch 00484: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0962 - mean_absolute_error: 0.0962 - val_loss: 0.0346 - val_mean_absolute_error: 0.0346\n",
            "Epoch 485/500\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.1237 - mean_absolute_error: 0.1237\n",
            "Epoch 00485: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1237 - mean_absolute_error: 0.1237 - val_loss: 0.1721 - val_mean_absolute_error: 0.1721\n",
            "Epoch 486/500\n",
            "181/188 [===========================>..] - ETA: 0s - loss: 0.0856 - mean_absolute_error: 0.0856\n",
            "Epoch 00486: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0862 - mean_absolute_error: 0.0862 - val_loss: 0.0486 - val_mean_absolute_error: 0.0486\n",
            "Epoch 487/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1509 - mean_absolute_error: 0.1509\n",
            "Epoch 00487: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1487 - mean_absolute_error: 0.1487 - val_loss: 0.1282 - val_mean_absolute_error: 0.1282\n",
            "Epoch 488/500\n",
            "179/188 [===========================>..] - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.0977\n",
            "Epoch 00488: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0997 - mean_absolute_error: 0.0997 - val_loss: 0.0805 - val_mean_absolute_error: 0.0805\n",
            "Epoch 489/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.0812 - mean_absolute_error: 0.0812\n",
            "Epoch 00489: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0812 - mean_absolute_error: 0.0812 - val_loss: 0.1016 - val_mean_absolute_error: 0.1016\n",
            "Epoch 490/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.0913 - mean_absolute_error: 0.0913\n",
            "Epoch 00490: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0906 - mean_absolute_error: 0.0906 - val_loss: 0.1634 - val_mean_absolute_error: 0.1634\n",
            "Epoch 491/500\n",
            "178/188 [===========================>..] - ETA: 0s - loss: 0.0841 - mean_absolute_error: 0.0841\n",
            "Epoch 00491: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0852 - mean_absolute_error: 0.0852 - val_loss: 0.2853 - val_mean_absolute_error: 0.2853\n",
            "Epoch 492/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1708 - mean_absolute_error: 0.1708\n",
            "Epoch 00492: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1726 - mean_absolute_error: 0.1726 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
            "Epoch 493/500\n",
            "180/188 [===========================>..] - ETA: 0s - loss: 0.1028 - mean_absolute_error: 0.1028\n",
            "Epoch 00493: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1046 - mean_absolute_error: 0.1046 - val_loss: 0.0941 - val_mean_absolute_error: 0.0941\n",
            "Epoch 494/500\n",
            "182/188 [============================>.] - ETA: 0s - loss: 0.1177 - mean_absolute_error: 0.1177\n",
            "Epoch 00494: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1166 - mean_absolute_error: 0.1166 - val_loss: 0.1133 - val_mean_absolute_error: 0.1133\n",
            "Epoch 495/500\n",
            "184/188 [============================>.] - ETA: 0s - loss: 0.1419 - mean_absolute_error: 0.1419\n",
            "Epoch 00495: val_loss did not improve from 0.02222\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1408 - mean_absolute_error: 0.1408 - val_loss: 0.0534 - val_mean_absolute_error: 0.0534\n",
            "Epoch 496/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.1263 - mean_absolute_error: 0.1263\n",
            "Epoch 00496: val_loss improved from 0.02222 to 0.02192, saving model to Weights-496--0.02192.hdf5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.1256 - mean_absolute_error: 0.1256 - val_loss: 0.0219 - val_mean_absolute_error: 0.0219\n",
            "Epoch 497/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0693 - mean_absolute_error: 0.0693\n",
            "Epoch 00497: val_loss did not improve from 0.02192\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.0688 - mean_absolute_error: 0.0688 - val_loss: 0.0913 - val_mean_absolute_error: 0.0913\n",
            "Epoch 498/500\n",
            "183/188 [============================>.] - ETA: 0s - loss: 0.1086 - mean_absolute_error: 0.1086\n",
            "Epoch 00498: val_loss did not improve from 0.02192\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.1113 - mean_absolute_error: 0.1113 - val_loss: 0.1256 - val_mean_absolute_error: 0.1256\n",
            "Epoch 499/500\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.0835 - mean_absolute_error: 0.0835\n",
            "Epoch 00499: val_loss did not improve from 0.02192\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0831 - mean_absolute_error: 0.0831 - val_loss: 0.1233 - val_mean_absolute_error: 0.1233\n",
            "Epoch 500/500\n",
            "185/188 [============================>.] - ETA: 0s - loss: 0.0849 - mean_absolute_error: 0.0849\n",
            "Epoch 00500: val_loss did not improve from 0.02192\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.0851 - mean_absolute_error: 0.0851 - val_loss: 0.0979 - val_mean_absolute_error: 0.0979\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9edd204d0>"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = NN_model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCgfv6U3QR2N",
        "outputId": "495f2b10-0cf1-48a4-aab5-46e34bcb6d54"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.1168237 ]\n",
            " [1.3723457 ]\n",
            " [0.92598075]\n",
            " ...\n",
            " [1.6706116 ]\n",
            " [0.6054829 ]\n",
            " [1.5349393 ]]\n"
          ]
        }
      ]
    }
  ]
}